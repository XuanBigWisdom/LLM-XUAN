{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df1d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'natural_questions' on the Hub (ProxyError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, PromptTuningConfig, PeftType\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 加载 Natural Questions 数据集 (子集)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnatural_questions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain[:5\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 加载 Qwen-0.5B 模型\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen-0.5B\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 替换为你的本地模型路径\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\datasets\\load.py:2074\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2069\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2070\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2071\u001b[0m )\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2074\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\datasets\\load.py:1795\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   1793\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1794\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 1795\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1808\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\datasets\\load.py:1671\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1666\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1667\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1668\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1669\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1670\u001b[0m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1671\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1675\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\datasets\\load.py:1591\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m hf_api\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[0;32m   1581\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1582\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1583\u001b[0m         token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m   1584\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[0;32m   1585\u001b[0m     )\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m   1587\u001b[0m     OfflineModeIsEnabled,\n\u001b[0;32m   1588\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectTimeout,\n\u001b[0;32m   1589\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m   1590\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[0;32m   1594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1595\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: Couldn't reach 'natural_questions' on the Hub (ProxyError)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PromptTuningConfig, PeftType\n",
    "\n",
    "# 加载 Natural Questions 数据集 (子集)\n",
    "dataset = load_dataset(\"natural_questions\", split='train[:5%]')\n",
    "\n",
    "# 加载 Qwen-0.5B 模型\n",
    "model_name = \"Qwen-0.5B\"  # 替换为你的本地模型路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# 将数据集进行Tokenization\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q.strip() for q in examples['question']]\n",
    "    targets = [a['text'][0].strip() for a in examples['answers']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b463ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'answer': 'Saint Bernadette Soubirous', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'context': 'It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.', 'p_phrase': ['Lourdes', 'France', '1858', 'the Virgin Mary', 'appeared'], 'n_phrase': ['Saint Bernadette Soubirous'], 'full answer': 'The Virgin Mary allegedly appeared in 1858 in Lourdes France to Saint Bernadette Soubirous.'}, {'id': 1, 'answer': 'a copper statue of Christ', 'question': 'What is in front of the Notre Dame Main Building?', 'context': 'Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".', 'p_phrase': ['the Main Building', 'Christ', 'front'], 'n_phrase': ['a copper statue', 'facing'], 'full answer': 'A copper statue of Christ is in front of the Notre Dame Main Building.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取JSON数据集并指定编码为utf-8\n",
    "with open('./data/SQuAD1.1/train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 拿出来看看结构\n",
    "print(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debad80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPast(last_hidden_state=tensor([[[ 5.9717,  1.8740,  4.2277,  ..., -0.1168, -1.7965, -0.2775],\n",
      "         [-2.5296, -0.5739, -6.1173,  ..., -6.6078, -5.1615, -2.7894]]],\n",
      "       grad_fn=<MulBackward0>), past_key_values=((tensor([[[[-1.0833e+00, -2.4329e-01, -2.5547e-01,  ..., -2.2154e+00,\n",
      "            2.2616e+00, -4.3567e+00],\n",
      "          [-1.2909e+00, -3.6945e-02, -1.7132e-01,  ..., -2.2704e+00,\n",
      "            2.2627e+00, -4.3512e+00]],\n",
      "\n",
      "         [[-3.1599e+01,  2.4552e+01,  1.4655e+01,  ..., -2.2617e+01,\n",
      "           -1.2975e+01, -1.4119e+01],\n",
      "          [ 1.3911e+01,  2.9846e+01,  1.7154e+01,  ..., -2.2602e+01,\n",
      "           -1.3038e+01, -1.4156e+01]],\n",
      "\n",
      "         [[ 8.4961e+00,  1.2535e+01, -6.8833e-02,  ..., -1.2001e+00,\n",
      "           -4.7996e+00, -8.3774e-01],\n",
      "          [ 4.0529e+00,  8.6046e+00, -7.6709e-02,  ..., -1.1763e+00,\n",
      "           -4.8314e+00, -8.0779e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0931e-02, -1.6246e-02,  8.1112e-02,  ..., -1.2140e+01,\n",
      "           -1.1836e+01, -1.0581e+01],\n",
      "          [ 6.7910e-02,  3.1513e-04, -6.1615e-02,  ..., -1.2180e+01,\n",
      "           -1.1876e+01, -1.0612e+01]],\n",
      "\n",
      "         [[-5.2685e-01, -1.2594e+00, -2.8519e+00,  ..., -1.1104e+01,\n",
      "            1.0374e+01, -7.1945e+00],\n",
      "          [ 1.7576e+00,  5.5244e-01, -4.4344e+00,  ..., -1.1040e+01,\n",
      "            1.0156e+01, -7.2466e+00]],\n",
      "\n",
      "         [[ 2.0107e+01,  1.5387e+01,  6.3211e+00,  ...,  1.9534e-01,\n",
      "            3.0108e-01, -1.9459e+00],\n",
      "          [ 3.2852e+01,  1.0868e+01,  8.3938e+00,  ...,  2.5239e-01,\n",
      "            2.3020e-01, -1.6972e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0158,  0.0172, -0.0158,  ...,  0.0413, -0.0103, -0.0055],\n",
      "          [ 0.0388,  0.0177, -0.0056,  ..., -0.0407,  0.0047,  0.0042]],\n",
      "\n",
      "         [[-0.0130, -0.0013,  0.0107,  ..., -0.0123, -0.0246, -0.0104],\n",
      "          [ 0.0372,  0.0500, -0.0134,  ..., -0.0093, -0.0072, -0.0110]],\n",
      "\n",
      "         [[-0.0231, -0.0009,  0.0124,  ..., -0.0351,  0.0307,  0.0137],\n",
      "          [-0.0080, -0.0091, -0.0080,  ..., -0.0032,  0.0117, -0.0013]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0005, -0.0009,  0.0444,  ...,  0.0239, -0.0007, -0.0398],\n",
      "          [-0.0028, -0.0442, -0.0636,  ..., -0.0191, -0.0248, -0.0147]],\n",
      "\n",
      "         [[-0.0030, -0.0619, -0.0161,  ...,  0.0481,  0.0259, -0.0048],\n",
      "          [ 0.0187,  0.0076, -0.0172,  ..., -0.0066,  0.0220, -0.0039]],\n",
      "\n",
      "         [[ 0.0124,  0.0240, -0.0121,  ..., -0.0005,  0.0214, -0.0123],\n",
      "          [ 0.0209,  0.0135, -0.0183,  ...,  0.0263,  0.0101, -0.0326]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-5.5214e-01,  7.0503e-01, -1.3136e-01,  ..., -3.1168e+00,\n",
      "           -4.2983e+00, -1.5165e+00],\n",
      "          [ 2.6504e-01, -2.0317e-01,  6.4744e-01,  ..., -2.7701e+00,\n",
      "           -2.8067e+00, -1.5189e+00]],\n",
      "\n",
      "         [[ 1.1198e+00,  7.3581e-01, -6.4711e-01,  ..., -5.3263e-01,\n",
      "           -7.0573e+00, -8.5434e+00],\n",
      "          [ 4.7792e-01,  1.0863e+00, -6.4945e-01,  ..., -2.0894e+00,\n",
      "           -8.3519e+00, -9.3375e+00]],\n",
      "\n",
      "         [[-2.0273e-01,  1.2861e+00,  6.4122e-01,  ...,  2.4161e+00,\n",
      "           -5.3443e-01,  3.7862e-01],\n",
      "          [ 2.0012e+00,  1.9824e+00, -8.0234e-01,  ...,  2.0703e+00,\n",
      "           -1.0408e+00,  8.9969e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2587e-01, -5.7437e-01, -3.6774e-01,  ...,  4.6265e+00,\n",
      "            2.5817e+00, -7.5092e-01],\n",
      "          [-1.1471e+00, -5.9461e-02,  4.3100e-01,  ...,  4.4350e+00,\n",
      "            1.6291e+00,  4.9537e-02]],\n",
      "\n",
      "         [[ 8.5965e-01,  5.7086e-01, -1.0739e+00,  ...,  1.4807e+00,\n",
      "           -5.1985e+00, -2.3217e+00],\n",
      "          [-4.4071e-01, -4.1500e-03, -1.0111e+00,  ...,  3.3484e+00,\n",
      "           -2.0470e+00, -2.4896e+00]],\n",
      "\n",
      "         [[ 2.6694e-02, -7.3820e-01, -5.3918e-01,  ...,  1.4416e+00,\n",
      "            2.6101e-01, -2.6898e+00],\n",
      "          [-6.2642e-01, -8.9894e-01, -5.9361e-01,  ...,  2.0475e+00,\n",
      "            6.3240e-02, -1.6962e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0619,  0.0414, -0.0423,  ...,  0.1651,  0.2419,  0.1328],\n",
      "          [-0.0981,  0.0786,  0.1052,  ...,  0.1401,  0.2111,  0.1058]],\n",
      "\n",
      "         [[ 0.1886,  0.0951,  0.3027,  ...,  0.0611,  0.2184,  0.1467],\n",
      "          [ 0.1898,  0.4251,  0.3740,  ..., -0.1437, -0.0161, -0.0013]],\n",
      "\n",
      "         [[ 0.0389, -0.0343,  0.0284,  ..., -0.0947,  0.1299,  0.0404],\n",
      "          [ 0.2503,  0.1346, -0.3722,  ...,  0.0080, -0.0858,  0.0785]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1480, -0.0986, -0.0017,  ..., -0.0632, -0.1764,  0.0449],\n",
      "          [ 0.3384,  0.1415, -0.2401,  ...,  0.2411, -0.0801,  0.0144]],\n",
      "\n",
      "         [[ 0.0496, -0.1033,  0.1264,  ..., -0.2516, -0.0919,  0.2768],\n",
      "          [-0.1869, -0.0635, -0.0453,  ...,  0.6917,  0.4824,  0.4678]],\n",
      "\n",
      "         [[-0.2313, -0.1281,  0.3779,  ...,  0.1163, -0.1743, -0.2298],\n",
      "          [-0.0050,  0.0852,  0.0013,  ..., -0.1105, -0.4322, -0.0285]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-3.8517e-01, -2.2894e-01, -1.4121e-01,  ..., -8.5614e-02,\n",
      "           -4.7774e+00, -6.6154e+00],\n",
      "          [ 9.4613e-02, -8.9246e-01,  6.0778e-03,  ..., -2.1626e+00,\n",
      "           -3.8365e+00, -6.5008e+00]],\n",
      "\n",
      "         [[ 2.6292e-01, -3.2446e-01, -2.8952e-01,  ...,  8.8356e-01,\n",
      "            2.0560e+00, -3.1210e+00],\n",
      "          [ 1.2895e-01, -1.4915e+00, -8.7862e-01,  ...,  7.8957e-01,\n",
      "            6.7780e-01, -3.5309e+00]],\n",
      "\n",
      "         [[-4.4318e-01,  2.0393e-01, -1.3962e-01,  ...,  6.6155e-01,\n",
      "           -1.0487e+00,  1.8489e+00],\n",
      "          [-1.3870e+00, -1.3571e+00, -1.1199e+00,  ...,  6.2663e-01,\n",
      "           -2.2362e+00, -7.5766e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0585e+00, -1.2612e+00, -6.0586e-01,  ..., -1.9852e+00,\n",
      "           -2.9428e+00, -6.2102e+00],\n",
      "          [ 1.7316e+00, -1.8822e+00, -5.7935e-01,  ..., -4.6371e+00,\n",
      "           -2.0730e+00, -5.8905e+00]],\n",
      "\n",
      "         [[-6.9096e-01, -1.9122e-01, -3.2977e-01,  ..., -1.2820e+00,\n",
      "           -9.5266e-01,  5.4277e+00],\n",
      "          [ 2.3883e-02,  9.7723e-02,  8.9649e-01,  ..., -1.1432e+00,\n",
      "            7.5401e-01,  6.4884e+00]],\n",
      "\n",
      "         [[ 4.4883e-01,  5.5003e-01,  4.3940e-01,  ...,  9.3119e-01,\n",
      "           -1.1823e+00,  4.6277e-01],\n",
      "          [-2.8776e-01,  3.2625e-01,  2.6847e-01,  ...,  1.1695e+00,\n",
      "           -5.8100e-01,  7.3866e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3921e-01, -1.0627e-01,  1.2655e-01,  ..., -1.6542e-02,\n",
      "            3.0498e-02,  2.0425e-01],\n",
      "          [-1.7528e-01,  3.2838e-01, -1.1964e-01,  ...,  1.1618e-01,\n",
      "            2.9623e-01,  1.1612e+00]],\n",
      "\n",
      "         [[ 1.9533e-02,  1.4251e-01, -2.0081e-01,  ...,  3.9347e-02,\n",
      "           -2.4919e-01, -2.6848e-01],\n",
      "          [ 1.1196e-01,  2.6265e-01, -2.8190e-01,  ..., -3.4863e-01,\n",
      "            2.2863e-01,  2.4697e-01]],\n",
      "\n",
      "         [[-4.7915e-04, -1.1467e-01, -1.0617e-01,  ..., -1.1518e-01,\n",
      "           -3.7484e-01,  7.4873e-02],\n",
      "          [ 4.2824e-01,  8.0600e-02,  5.3264e-02,  ...,  2.3523e-01,\n",
      "           -1.6877e-01, -6.3750e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.6507e-02, -1.8909e-01,  2.8076e-01,  ...,  1.7069e-02,\n",
      "           -7.4531e-02, -1.8331e-01],\n",
      "          [-8.7785e-02, -1.2009e-01,  4.8558e-01,  ...,  4.7343e-01,\n",
      "            1.7614e-01,  9.2937e-02]],\n",
      "\n",
      "         [[-1.2206e-01, -1.9239e-01,  2.8323e-01,  ...,  5.2112e-02,\n",
      "            7.2598e-02, -3.0211e-02],\n",
      "          [ 6.0224e-01, -4.3845e-02,  4.3880e-01,  ...,  3.9198e-01,\n",
      "           -2.5423e-01, -1.1599e-02]],\n",
      "\n",
      "         [[ 4.4575e-02,  1.7815e-01, -1.4582e-03,  ..., -2.0066e-01,\n",
      "            1.2504e-01,  5.6023e-02],\n",
      "          [-3.1217e-01, -1.6014e-02,  2.5169e-02,  ...,  1.9068e-01,\n",
      "            8.2331e-02,  1.0285e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.3665,  0.0818,  0.2419,  ...,  2.8979, -2.0685, -4.2934],\n",
      "          [-1.8364,  1.3474,  0.7003,  ...,  3.3995, -1.3050, -1.6753]],\n",
      "\n",
      "         [[ 0.7464, -1.0814,  0.5891,  ..., -0.2494,  0.0424,  0.8571],\n",
      "          [-1.2637, -2.0455,  0.7167,  ..., -0.9573,  1.4764,  2.6102]],\n",
      "\n",
      "         [[-0.1816, -0.8262, -0.4555,  ...,  3.6775,  1.9476, -2.0363],\n",
      "          [-0.6899,  0.2586, -0.3173,  ...,  4.8946,  1.1931, -2.2171]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1897,  2.3950,  0.5076,  ..., -0.4901,  3.3577,  3.0368],\n",
      "          [ 1.9344, -0.6871, -1.3755,  ...,  1.0044,  2.6304,  3.6952]],\n",
      "\n",
      "         [[-1.1560,  0.6363, -1.1084,  ..., -4.3256,  0.3856, -4.9183],\n",
      "          [ 0.3731, -0.1518, -1.0214,  ..., -5.2105,  2.3473, -5.4305]],\n",
      "\n",
      "         [[ 0.0609, -0.5463, -2.1055,  ...,  1.1229, -0.0855, -1.7712],\n",
      "          [-0.5379, -2.0881, -2.2848,  ...,  0.1667, -1.1362, -2.4099]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[ 0.1733, -0.2221, -0.1289,  ..., -0.0370, -0.0364, -0.1551],\n",
      "          [ 0.1888, -0.5019, -0.2267,  ...,  0.2192, -0.3324, -0.2959]],\n",
      "\n",
      "         [[-0.0219,  0.1958,  0.2507,  ..., -0.2872, -0.1897, -0.0935],\n",
      "          [-0.0524,  0.2252, -0.2796,  ...,  0.2200, -0.3783,  0.3242]],\n",
      "\n",
      "         [[-0.9639,  0.2695,  0.0705,  ...,  0.3087,  0.6955, -0.2284],\n",
      "          [ 0.8711, -0.0122, -0.0940,  ...,  0.0539,  2.1203,  0.5549]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3089, -0.8805, -0.6467,  ...,  0.7608, -0.3787,  0.7900],\n",
      "          [ 0.1549,  0.0570,  0.0272,  ..., -0.2099,  0.7278, -0.1298]],\n",
      "\n",
      "         [[ 0.0094, -0.0652, -0.1253,  ...,  0.3440,  0.0332,  0.2002],\n",
      "          [ 0.1465, -0.4273,  0.0617,  ..., -0.4215, -0.3908,  0.4385]],\n",
      "\n",
      "         [[ 0.0671,  0.0276, -0.0947,  ...,  0.0940,  0.2597, -0.0764],\n",
      "          [ 0.0360,  0.2962, -0.2596,  ...,  0.1532,  0.1962, -0.4950]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.8811,  0.7550,  5.0600,  ..., -1.0145,  2.1537,  0.4105],\n",
      "          [ 4.7475,  2.2059,  3.5310,  ..., -3.3546,  2.8616, -0.0464]],\n",
      "\n",
      "         [[ 4.4496,  1.4836,  2.8194,  ..., -4.2694,  1.8389,  1.7116],\n",
      "          [ 1.9032, -2.4487, -0.1775,  ..., -2.2786,  2.3772,  0.7233]],\n",
      "\n",
      "         [[ 0.2084, -1.5268, -2.3746,  ..., -1.2524,  3.9559, -3.7568],\n",
      "          [-3.6063,  0.6428, -3.0136,  ..., -1.3551,  4.0578, -2.4635]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7134, -0.4309, -0.6427,  ..., -2.1824,  5.9470, -3.9613],\n",
      "          [ 1.1208, -0.7317,  0.3977,  ..., -0.2261,  4.4981, -3.1589]],\n",
      "\n",
      "         [[-1.8361, -1.7336,  0.2580,  ..., -1.5708, -0.6102,  0.8986],\n",
      "          [ 2.4145, -0.6455,  0.4587,  ..., -1.7645,  1.1092, -0.4766]],\n",
      "\n",
      "         [[ 0.7942, -0.5103,  1.2453,  ...,  3.8021,  0.5815,  1.9812],\n",
      "          [-4.3423,  1.0718, -0.2227,  ...,  3.8117,  0.8322,  1.2131]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[-0.0618,  0.1685,  0.1219,  ...,  0.1665, -0.2980, -0.3103],\n",
      "          [ 0.1887, -0.0181,  0.0860,  ...,  0.1179,  0.0566,  0.0542]],\n",
      "\n",
      "         [[ 0.0962,  0.0627, -0.1753,  ...,  0.3129, -0.0578,  0.2870],\n",
      "          [-0.0372, -0.3001, -0.2277,  ...,  0.4359, -0.2843, -0.2468]],\n",
      "\n",
      "         [[ 0.0497,  0.0049,  0.1403,  ...,  0.1636, -0.0765,  0.1479],\n",
      "          [ 0.2678, -0.2019,  0.1548,  ...,  0.2542,  0.3594,  0.4581]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2970, -0.4940, -0.7151,  ..., -0.1601,  0.4021,  0.0547],\n",
      "          [ 0.3164, -0.1322,  0.3195,  ..., -0.0223,  0.1305,  0.1893]],\n",
      "\n",
      "         [[ 0.5165, -0.1103, -0.1275,  ..., -0.1824,  0.2249,  0.3014],\n",
      "          [ 0.1281,  0.2020, -0.0752,  ...,  0.1675, -0.2051, -0.3448]],\n",
      "\n",
      "         [[ 0.1721,  0.3042,  0.0536,  ...,  0.1416, -0.0287, -0.0346],\n",
      "          [-0.3649, -0.0028,  0.3185,  ...,  0.0563,  0.5960,  0.0809]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0672, -0.2379, -0.3427,  ..., -7.9007,  6.2929, 17.6751],\n",
      "          [-0.3804, -1.5812, -0.1848,  ..., -9.1035,  3.4655, 17.4881]],\n",
      "\n",
      "         [[-0.7244, -0.2221, -1.1155,  ...,  3.4691,  0.9771,  3.7577],\n",
      "          [-1.0831,  0.7497,  0.9791,  ...,  3.9073,  1.8414,  0.3569]],\n",
      "\n",
      "         [[ 0.0789,  0.3851, -0.0478,  ...,  0.3856, -8.7848,  0.3213],\n",
      "          [-0.4322,  0.3859, -0.1642,  ...,  1.9302, -7.4227,  2.3604]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4302, -0.3714, -0.1405,  ...,  1.9723, -2.5913,  0.8739],\n",
      "          [ 0.3492, -0.1655, -0.7391,  ...,  2.8254, -4.2536,  1.3119]],\n",
      "\n",
      "         [[ 0.8009, -1.7015,  1.7374,  ..., -2.9057, -1.4876, -0.5464],\n",
      "          [-5.0103,  1.3239,  2.4215,  ..., -2.5762, -3.9276,  0.0229]],\n",
      "\n",
      "         [[-0.1369, -0.4340, -0.7519,  ...,  3.5067,  5.1386,  1.7118],\n",
      "          [-1.0016, -3.7124, -3.2793,  ...,  0.3151,  6.1888,  0.0670]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[-0.1587, -0.1151, -0.3133,  ..., -0.3298,  0.1195, -0.2815],\n",
      "          [-0.4699,  0.5676,  0.1969,  ..., -0.1319, -0.4672,  0.5281]],\n",
      "\n",
      "         [[-0.0736,  0.1694,  0.1844,  ..., -0.0099,  0.3159, -0.0867],\n",
      "          [ 0.0392,  0.0279, -0.0861,  ..., -0.0911,  0.1656,  0.0832]],\n",
      "\n",
      "         [[-0.2752,  0.2390,  0.0216,  ..., -0.2871,  0.1049,  0.0828],\n",
      "          [-0.4271,  0.3893, -0.2052,  ..., -0.7396,  0.3539,  0.8598]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0332, -0.1223, -0.1500,  ...,  0.0065, -0.2405,  0.0170],\n",
      "          [-0.1068, -0.4266,  0.0402,  ...,  0.2503,  0.0753, -0.1781]],\n",
      "\n",
      "         [[-0.1447,  0.1200, -0.2215,  ..., -0.1342,  0.0691,  0.1182],\n",
      "          [-0.0772,  0.1139,  0.1581,  ...,  0.2213, -0.4733,  0.1374]],\n",
      "\n",
      "         [[-0.5699, -0.2334,  0.0801,  ..., -0.4534, -0.0692, -0.0207],\n",
      "          [-0.5500, -0.4599,  0.0198,  ..., -0.3695,  0.3692, -0.0564]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.3494,  0.2755, -0.0688,  ...,  0.7119,  1.0306, -1.6579],\n",
      "          [-2.3880,  1.6204, -0.2284,  ...,  0.5363, -1.3396, -2.1843]],\n",
      "\n",
      "         [[ 0.1369, -0.0681,  0.1579,  ..., -0.1602, -0.0997, -0.6784],\n",
      "          [ 2.9096, -0.5556,  2.3231,  ..., -0.9910, -0.6400, -4.9159]],\n",
      "\n",
      "         [[ 0.2769, -0.2709, -0.2631,  ..., -2.6779,  0.5099, -0.5497],\n",
      "          [ 2.6526, -1.9170, -2.1132,  ..., -0.6544,  0.9029,  0.0546]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2572,  0.0843, -0.2607,  ...,  2.8684, -1.0708, -0.1362],\n",
      "          [-0.0739,  0.7461,  0.1059,  ..., -1.6578, -0.9336, -1.1509]],\n",
      "\n",
      "         [[-0.1579,  0.0422,  0.2274,  ..., -2.1561, -2.0863,  2.8851],\n",
      "          [-0.3140, -1.1064,  0.2967,  ..., -3.2147, -0.6651,  3.4849]],\n",
      "\n",
      "         [[ 0.0425,  0.0518, -0.2259,  ..., -0.3949,  1.0326, -0.2727],\n",
      "          [-0.1180,  0.8919, -2.4832,  ..., -1.2838, -4.9541, -0.6599]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[-2.3765e-02, -1.2401e-02, -1.2983e-03,  ...,  3.5570e-02,\n",
      "            1.0355e-02,  3.9602e-03],\n",
      "          [-2.4584e-02,  6.0932e-02, -2.5134e-03,  ..., -2.5564e-01,\n",
      "            1.7669e-01, -7.4654e-01]],\n",
      "\n",
      "         [[ 2.1744e-02,  2.1717e-02,  3.7232e-03,  ...,  2.6571e-03,\n",
      "           -1.7919e-02, -1.5318e-02],\n",
      "          [-9.1373e-03,  7.1363e-02, -3.9138e-01,  ...,  4.1295e-01,\n",
      "            4.3597e-01, -5.1280e-01]],\n",
      "\n",
      "         [[-1.1930e-02, -2.8277e-02, -5.7007e-03,  ...,  1.7011e-02,\n",
      "           -3.4798e-02,  6.4983e-04],\n",
      "          [ 3.8243e-01,  2.4288e-01, -2.2460e-01,  ...,  4.0601e-01,\n",
      "            2.0278e-01,  7.8399e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3219e-02, -1.9410e-02, -1.5037e-02,  ..., -2.7120e-02,\n",
      "            1.3397e-02, -1.8531e-02],\n",
      "          [-3.3941e-01, -1.7175e-01,  6.9700e-02,  ...,  3.0810e-01,\n",
      "            1.1354e-02, -3.1304e-02]],\n",
      "\n",
      "         [[ 1.0414e-02,  1.5701e-01,  1.2303e-02,  ..., -2.8653e-03,\n",
      "           -4.8033e-02, -3.9300e-02],\n",
      "          [-5.5871e-02, -1.3435e+00, -1.5904e-01,  ..., -8.7030e-01,\n",
      "            7.2961e-01, -3.2392e-01]],\n",
      "\n",
      "         [[-3.3284e-03,  2.8572e-02, -3.2445e-02,  ..., -2.3660e-02,\n",
      "           -4.4711e-03,  1.6342e-02],\n",
      "          [ 9.0155e-01,  7.6237e-01, -5.4415e-01,  ..., -3.9394e-01,\n",
      "           -1.4160e-01, -2.6472e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7458e-01, -1.0703e-02, -9.8094e-03,  ...,  9.9469e-02,\n",
      "           -1.1275e-01,  5.7948e-01],\n",
      "          [-4.5875e-01,  1.6986e+00, -1.2312e-01,  ...,  9.9351e-01,\n",
      "            4.7306e-01, -1.3165e+00]],\n",
      "\n",
      "         [[-7.5457e-02, -1.6206e-01, -9.5439e-02,  ...,  3.8082e-01,\n",
      "           -5.4371e-01, -6.0465e-01],\n",
      "          [-8.8324e-01,  2.6778e-01,  3.0069e-01,  ...,  4.5327e-02,\n",
      "           -5.5577e-01, -5.6003e-01]],\n",
      "\n",
      "         [[ 1.1596e-02,  6.5428e-03,  1.4454e-02,  ...,  1.3646e+00,\n",
      "            3.8785e-01, -4.1020e-01],\n",
      "          [ 3.1973e-01, -1.4741e+00, -6.4224e-01,  ...,  3.8829e+00,\n",
      "           -7.1093e-04,  1.3842e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.1018e-02,  1.3718e-01, -8.7670e-02,  ..., -1.2728e-01,\n",
      "            6.5197e-01, -3.7176e-01],\n",
      "          [-1.3344e+00,  5.1869e-01,  6.2136e-01,  ..., -2.0947e+00,\n",
      "            2.6185e+00, -3.2170e+00]],\n",
      "\n",
      "         [[-5.9975e-02, -1.6581e-01,  8.7132e-03,  ...,  2.4293e-02,\n",
      "           -2.0239e-01, -2.5236e-01],\n",
      "          [ 1.0655e+00, -2.8235e+00, -3.8999e-01,  ..., -5.2425e-01,\n",
      "           -3.1517e-01, -4.3328e-01]],\n",
      "\n",
      "         [[-6.3740e-02,  1.5548e-01,  5.2956e-03,  ..., -4.9393e-01,\n",
      "            2.2706e-01,  1.2615e-01],\n",
      "          [-6.1858e-01,  2.8341e+00, -6.8995e-01,  ..., -4.3633e-01,\n",
      "            1.0110e+00, -2.4867e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0464,  0.0384,  0.0338,  ...,  0.0076, -0.0366,  0.0331],\n",
      "          [ 0.7237,  0.8346, -0.1880,  ..., -0.5808, -0.2200, -0.0617]],\n",
      "\n",
      "         [[ 0.0309, -0.0456, -0.0147,  ..., -0.0149,  0.0063, -0.0171],\n",
      "          [ 0.2818,  0.5061,  0.5724,  ...,  1.3404, -0.9744,  0.6133]],\n",
      "\n",
      "         [[ 0.0195, -0.0240, -0.0227,  ..., -0.0191,  0.0289, -0.0050],\n",
      "          [ 0.0141, -0.8378,  0.4746,  ...,  1.3062, -0.5125,  0.0105]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0117,  0.0142, -0.0399,  ..., -0.0543,  0.0170, -0.0678],\n",
      "          [ 0.0879, -0.2493,  0.0327,  ...,  0.4222, -0.0755, -0.7917]],\n",
      "\n",
      "         [[ 0.0183, -0.0372, -0.0261,  ..., -0.0489, -0.0174, -0.0076],\n",
      "          [ 0.0911,  0.0059, -0.0216,  ..., -0.3964, -0.0557,  0.2985]],\n",
      "\n",
      "         [[-0.0288, -0.1085, -0.0117,  ..., -0.0346, -0.0056, -0.0324],\n",
      "          [ 0.2085,  1.6270, -0.3020,  ...,  0.6372, -0.0923,  0.2018]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.5807e-01, -1.0004e-01, -1.5306e-01,  ..., -1.3242e+00,\n",
      "            1.0755e+00, -4.6561e+00],\n",
      "          [-1.6344e+00, -2.1931e+00, -2.1005e+00,  ..., -1.5406e+00,\n",
      "            1.0908e+00, -5.2718e+00]],\n",
      "\n",
      "         [[ 3.0380e-02, -1.7926e-02, -1.1730e-02,  ..., -1.7908e+00,\n",
      "           -3.2660e+00, -9.8698e-01],\n",
      "          [-2.4839e+00, -3.1628e+00,  2.8609e+00,  ..., -6.2182e-01,\n",
      "           -2.1164e+00, -8.7819e-01]],\n",
      "\n",
      "         [[-2.0010e-01, -9.7789e-02,  1.2249e-01,  ...,  1.7818e-01,\n",
      "           -1.8491e-01,  1.9935e-01],\n",
      "          [-7.1944e-01, -1.8329e+00,  1.3874e+00,  ..., -1.4133e+00,\n",
      "            9.0638e-01, -2.8562e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9713e-02, -4.1437e-02, -3.8012e-02,  ...,  1.1742e+00,\n",
      "           -2.5820e-01, -8.4906e-02],\n",
      "          [-2.8141e+00,  7.1230e-01, -1.3430e+00,  ...,  2.6187e+00,\n",
      "           -1.9254e-01, -3.0853e-01]],\n",
      "\n",
      "         [[ 9.4180e-02,  8.6198e-02, -6.3122e-02,  ..., -8.6126e-02,\n",
      "            5.6277e+00,  1.1381e-01],\n",
      "          [-1.0177e+00,  8.2298e-01, -2.7811e+00,  ..., -4.3849e-01,\n",
      "            1.4994e+00, -2.4843e-01]],\n",
      "\n",
      "         [[-3.2692e-02,  6.1886e-02, -1.0191e-02,  ..., -3.4828e-02,\n",
      "            9.7967e-02, -2.6659e-01],\n",
      "          [-6.9716e-01, -1.1945e-04, -1.9007e-01,  ..., -9.2926e-01,\n",
      "            1.1071e-01, -6.9862e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-2.5655e-02,  9.5941e-02,  6.0312e-02,  ...,  3.3235e-02,\n",
      "            4.3514e-03, -4.4487e-02],\n",
      "          [ 4.4562e-01, -2.8773e-01,  2.5003e-01,  ...,  2.1946e-01,\n",
      "            4.9788e-01, -1.0665e-02]],\n",
      "\n",
      "         [[-8.7740e-02,  1.6478e-02, -2.9862e-02,  ..., -1.4106e-04,\n",
      "           -7.5149e-03,  3.2631e-02],\n",
      "          [-6.5957e-01, -7.9892e-03, -2.6433e-01,  ..., -2.9448e-01,\n",
      "           -4.6166e-01, -2.5086e-01]],\n",
      "\n",
      "         [[-1.2189e-02,  3.5642e-02, -3.5317e-02,  ..., -3.0904e-02,\n",
      "            5.8282e-02, -1.9488e-02],\n",
      "          [ 4.1425e-01, -2.4093e-01, -4.5594e-01,  ..., -8.6143e-02,\n",
      "            4.1700e-01,  4.7783e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7851e-03,  3.2027e-02, -1.9687e-01,  ...,  9.2105e-03,\n",
      "           -3.1529e-02,  1.4630e-02],\n",
      "          [-1.9885e-01,  3.4771e-01,  2.2822e-01,  ...,  2.4010e-01,\n",
      "            4.0185e-01,  1.9314e-01]],\n",
      "\n",
      "         [[-3.7786e-02,  7.3008e-04,  2.0829e-02,  ..., -9.8023e-04,\n",
      "           -2.5850e-02,  1.6360e-02],\n",
      "          [ 2.8819e-01,  2.7140e-01,  5.2708e-01,  ...,  4.8279e-02,\n",
      "            2.8210e-01,  1.0562e-02]],\n",
      "\n",
      "         [[-2.5792e-02,  4.0091e-02,  2.2990e-02,  ..., -3.9647e-03,\n",
      "           -1.0135e-02, -7.4839e-03],\n",
      "          [-2.2413e-01,  4.1249e-01,  3.1525e-01,  ..., -3.8051e-01,\n",
      "           -4.4727e-01,  1.7186e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[  3.6606,   2.2053,   2.7388,  ...,  -0.8027,   0.5833,  -0.1428],\n",
      "          [ -5.8269,  -1.6593,   3.9659,  ...,   1.1114,   3.1052,   0.4752]],\n",
      "\n",
      "         [[  0.0355,   0.0680,  -0.0829,  ...,  -0.2212,  -0.1396,   0.0423],\n",
      "          [ -0.8033,   1.8125,  -2.3725,  ...,   0.6739,   0.6856,   0.6499]],\n",
      "\n",
      "         [[ -0.0621,  -0.1140,  -0.0225,  ...,   1.8821,   0.3556,  -0.3087],\n",
      "          [ -2.6278,  -2.1854,  -1.7785,  ...,   2.4261,  -0.1389,  -1.7999]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  7.7943,  -6.7527,  -0.5689,  ...,  -3.4301,   6.6297, -10.9066],\n",
      "          [ -0.8307,   0.4663,  -1.2098,  ...,  -5.0357,   5.9469, -12.5301]],\n",
      "\n",
      "         [[ -0.0989,  -0.0399,   0.0563,  ...,   0.0283,   0.0667,   0.3461],\n",
      "          [ -1.6327,  -1.5219,   2.8176,  ...,  -0.3926,  -1.0677,   2.4436]],\n",
      "\n",
      "         [[ -0.0601,  -0.0371,   0.0126,  ...,   0.1140,  -0.1832,  -0.5644],\n",
      "          [  1.4886,   1.8818,  -1.7749,  ...,   3.4780,  -3.3268,   2.3577]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[ 0.0550,  0.0404,  0.0536,  ...,  0.0421,  0.0523,  0.1644],\n",
      "          [ 0.3155, -0.0478,  0.2391,  ..., -0.4914, -0.0672,  0.5349]],\n",
      "\n",
      "         [[-0.0169,  0.0128, -0.0129,  ...,  0.0214, -0.0398, -0.0475],\n",
      "          [-0.1749,  0.4108,  0.0056,  ..., -0.0270, -0.0073,  0.1288]],\n",
      "\n",
      "         [[ 0.0192,  0.0581,  0.0034,  ...,  0.0153,  0.0894,  0.0482],\n",
      "          [ 0.1775, -0.2272,  0.5106,  ..., -0.5644, -0.1258, -0.3569]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2009, -0.4558, -0.3799,  ..., -0.5233, -0.5659,  0.4273],\n",
      "          [ 0.6558, -0.3979,  0.0091,  ..., -1.1340,  0.0275, -0.4866]],\n",
      "\n",
      "         [[ 0.0097, -0.0368,  0.0051,  ..., -0.0167, -0.0351,  0.0231],\n",
      "          [ 0.4155, -0.9844, -0.7568,  ..., -0.1742,  0.3632,  0.1620]],\n",
      "\n",
      "         [[-0.0360, -0.0188,  0.0337,  ..., -0.0135,  0.0183,  0.0208],\n",
      "          [-0.4119,  0.6475,  0.2190,  ...,  0.7106,  0.1118, -0.4526]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.4114e-01,  4.4555e-02, -2.7513e-02,  ...,  3.2160e-01,\n",
      "           -6.8173e-01,  8.6262e-01],\n",
      "          [-4.2771e-01, -8.3679e-01, -5.6940e-01,  ...,  8.3661e-01,\n",
      "           -7.5272e+00,  1.5380e+00]],\n",
      "\n",
      "         [[ 1.2221e-02,  2.7864e-02, -2.9385e-02,  ..., -8.7193e-01,\n",
      "           -2.2448e-01,  4.3308e-02],\n",
      "          [ 5.2059e-01,  3.8319e+00,  2.1335e+00,  ...,  3.0630e+00,\n",
      "           -7.7492e-01, -9.9232e-03]],\n",
      "\n",
      "         [[-3.1295e-02, -1.2792e-02, -1.1841e-02,  ...,  2.0209e-01,\n",
      "           -2.2913e-01, -3.0210e-01],\n",
      "          [-6.1443e-01,  6.9464e-01,  1.7546e+00,  ..., -5.7356e-01,\n",
      "           -1.0460e+00,  3.5379e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.6797e-02,  1.1087e-02,  2.9525e-03,  ...,  1.0438e-01,\n",
      "            1.5834e-01,  1.0806e-01],\n",
      "          [ 4.5694e-01,  1.6193e+00,  3.9755e-01,  ..., -1.8066e+00,\n",
      "            5.6331e-01, -1.0219e+00]],\n",
      "\n",
      "         [[-5.6144e-02,  1.4835e-02, -8.7596e-03,  ..., -3.4813e-01,\n",
      "           -6.9163e-01,  1.4315e+00],\n",
      "          [-3.9779e-01, -8.3862e-01,  2.3861e-02,  ...,  4.9390e+00,\n",
      "            1.1798e-01, -2.1149e+00]],\n",
      "\n",
      "         [[-4.9024e-02, -4.0060e-02, -4.9714e-02,  ...,  3.0125e-02,\n",
      "            3.9216e-02, -5.3297e-01],\n",
      "          [-2.9179e+00, -4.8054e-01, -1.1751e+00,  ..., -1.1515e-01,\n",
      "            1.4229e+00, -2.3452e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.4523e-02,  5.0509e-02,  1.0249e-02,  ..., -2.1505e-02,\n",
      "            4.5163e-04, -3.6432e-02],\n",
      "          [ 1.9636e-01,  6.1264e-01,  1.0870e+00,  ..., -4.0764e-01,\n",
      "            4.7706e-01, -5.9261e-01]],\n",
      "\n",
      "         [[-5.2081e-02,  3.9523e-02,  1.8147e-02,  ..., -1.3525e-02,\n",
      "           -3.7178e-02,  4.2350e-02],\n",
      "          [ 4.2973e-01,  1.2602e-01, -8.6547e-01,  ..., -4.5823e-01,\n",
      "            5.7472e-01, -6.2973e-01]],\n",
      "\n",
      "         [[ 9.4606e-03,  5.5614e-03, -7.4804e-03,  ...,  2.6564e-02,\n",
      "           -1.9161e-02,  9.6222e-03],\n",
      "          [ 1.5582e-02, -2.0031e-01,  1.0618e+00,  ...,  4.0777e-01,\n",
      "           -3.2239e-01,  6.9821e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7438e-03,  9.8085e-03, -4.0451e-02,  ...,  5.2873e-02,\n",
      "           -3.3663e-02, -1.6824e-02],\n",
      "          [-5.8108e-01,  3.5792e-01, -3.8306e-01,  ...,  2.0263e-01,\n",
      "            1.0028e+00,  8.7988e-01]],\n",
      "\n",
      "         [[ 5.6223e-02, -2.3817e-02, -2.3376e-02,  ..., -4.3368e-02,\n",
      "           -1.2574e-02, -7.0770e-02],\n",
      "          [ 1.3282e-01,  3.2533e-01, -5.1378e-01,  ...,  3.4281e-01,\n",
      "            2.8904e-01,  9.2944e-01]],\n",
      "\n",
      "         [[ 6.1228e-02,  2.1739e-02, -1.8630e-02,  ..., -1.4296e-02,\n",
      "           -7.9588e-02, -1.2875e-02],\n",
      "          [-3.9553e-02,  1.8484e-02,  2.9519e-01,  ...,  8.9584e-02,\n",
      "            6.7501e-01, -4.0793e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.1798e-01, -6.4767e-02, -5.0369e-02,  ...,  1.1509e+00,\n",
      "           -3.9182e+00,  3.6336e-01],\n",
      "          [-7.2739e-01, -4.8017e-01, -1.1061e+00,  ..., -5.3992e-01,\n",
      "           -7.7911e-01,  1.7269e+00]],\n",
      "\n",
      "         [[ 2.8220e-02, -1.5386e-02, -3.8082e-02,  ..., -3.8271e-01,\n",
      "            2.7719e-01,  1.2604e+00],\n",
      "          [ 6.3523e+00,  1.5579e+00,  2.6832e+00,  ..., -3.1485e-01,\n",
      "           -5.4801e-01,  1.3043e+00]],\n",
      "\n",
      "         [[-7.4979e-02, -1.7202e-02, -1.2815e-02,  ..., -1.6317e-01,\n",
      "            3.5929e-01,  3.5428e-01],\n",
      "          [ 1.3940e+00, -1.1667e+00,  5.3520e-01,  ...,  1.8950e+00,\n",
      "           -1.6044e+00, -2.1140e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6174e-02,  1.1606e-01,  3.2809e-04,  ...,  2.6759e-01,\n",
      "            3.5882e-01, -2.2613e-01],\n",
      "          [-1.5797e+00,  1.5380e+00,  1.1764e-02,  ...,  1.2071e+00,\n",
      "            1.5850e+00,  1.7391e+00]],\n",
      "\n",
      "         [[-4.4706e-02, -5.6441e-02,  5.8466e-02,  ...,  2.2547e+00,\n",
      "           -4.5237e-01,  2.7063e+00],\n",
      "          [ 1.0945e+00, -1.3516e+00, -5.8387e-01,  ...,  2.2854e+00,\n",
      "           -2.6097e+00,  2.8303e+00]],\n",
      "\n",
      "         [[-6.2487e-02, -1.5237e-01,  1.0956e-01,  ...,  8.0616e-02,\n",
      "            1.0364e+00,  1.0838e+00],\n",
      "          [-4.5920e-01, -3.2803e-01,  1.5101e-01,  ..., -1.7936e+00,\n",
      "           -1.5801e-01,  5.4612e-02]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.9013e-01, -1.5056e-02,  5.1308e-02,  ..., -1.3005e-02,\n",
      "           -2.9893e-02,  2.0879e-01],\n",
      "          [ 1.2963e+00, -2.1102e-01, -2.9834e-01,  ..., -7.2699e-01,\n",
      "           -9.6061e-02, -1.6627e+00]],\n",
      "\n",
      "         [[-2.0956e-01,  6.1060e-02,  6.8382e-04,  ..., -2.4630e-02,\n",
      "            6.6770e-02, -3.1711e-02],\n",
      "          [-4.4404e-01, -4.5008e-01,  3.1368e-01,  ..., -1.0302e-02,\n",
      "            3.3970e-01,  3.0148e-01]],\n",
      "\n",
      "         [[ 1.8689e-02, -3.1973e-02, -1.9795e-02,  ...,  3.3538e-02,\n",
      "           -4.2335e-02,  6.1913e-03],\n",
      "          [-6.8702e-01, -2.3056e-02, -3.8464e-01,  ...,  6.6064e-01,\n",
      "           -2.6150e-01, -1.0328e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0176e-02,  5.7384e-02, -1.0102e-01,  ...,  6.5441e-03,\n",
      "           -3.4341e-02,  1.0131e-02],\n",
      "          [-4.9829e-01, -7.4736e-01, -4.2801e-01,  ..., -5.6700e-01,\n",
      "            8.3058e-01,  2.5367e-01]],\n",
      "\n",
      "         [[-6.7235e-02, -3.0979e-01, -4.0957e-01,  ...,  6.8314e-01,\n",
      "            2.9850e-01, -3.6274e-01],\n",
      "          [ 1.9506e-02,  3.8100e-01, -9.9704e-01,  ..., -1.7703e-01,\n",
      "           -6.8757e-01,  5.7459e-01]],\n",
      "\n",
      "         [[ 2.4994e-02, -3.3905e-03,  8.0547e-03,  ..., -3.4771e-02,\n",
      "            1.0086e-02, -2.7247e-01],\n",
      "          [ 3.3443e-01,  1.1511e-01,  4.1423e-02,  ...,  4.0523e-01,\n",
      "           -4.0658e-01, -1.0811e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 5.0566e-02, -7.2967e-02, -2.1162e-02,  ...,  9.0662e-02,\n",
      "            1.7683e-01,  4.2161e-01],\n",
      "          [-9.5767e-01, -3.2259e-01, -1.2370e+00,  ..., -1.9508e+00,\n",
      "           -2.1948e-01, -7.9433e+00]],\n",
      "\n",
      "         [[-7.0696e-02,  2.3147e-02, -6.3640e-02,  ..., -3.6901e-02,\n",
      "            1.9034e-01,  6.9527e-02],\n",
      "          [ 6.7001e-01,  3.6872e-02, -4.7494e-01,  ...,  8.3016e-01,\n",
      "           -7.6998e-01, -2.1231e-01]],\n",
      "\n",
      "         [[ 1.2257e-02,  3.9451e-02, -4.5339e-03,  ...,  1.0322e-01,\n",
      "            7.5695e-02, -3.1840e-01],\n",
      "          [-6.4008e-01,  1.3672e-01, -8.9967e-01,  ..., -1.1695e+00,\n",
      "           -3.5539e+00, -2.9075e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.1723e-02,  2.6265e-02, -1.6681e-02,  ..., -3.0121e-02,\n",
      "            2.1814e-01, -6.6827e+00],\n",
      "          [ 7.0987e-01, -2.7735e+00,  3.2943e+00,  ...,  1.0357e+00,\n",
      "            3.6436e-01, -1.0836e+00]],\n",
      "\n",
      "         [[-6.7324e-04,  1.6240e-02, -3.3334e-02,  ...,  5.5917e-01,\n",
      "           -2.6403e-01,  3.3960e-01],\n",
      "          [-5.9905e-01,  7.3924e-01, -7.0646e-02,  ..., -5.1633e+00,\n",
      "           -2.7373e+00,  1.3295e+00]],\n",
      "\n",
      "         [[ 4.8314e-02, -6.0415e-02, -3.0084e-02,  ...,  9.3643e-02,\n",
      "            2.7280e-01,  1.5516e-01],\n",
      "          [ 1.2587e+00, -2.3507e-01, -1.1102e+00,  ..., -6.4200e-01,\n",
      "            6.6159e-01,  4.8624e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.6191e-02, -1.9926e-02, -2.0381e-02,  ...,  6.6288e-02,\n",
      "           -7.9291e-03, -3.2027e-02],\n",
      "          [-7.5827e-01,  1.5417e-01, -1.8380e+00,  ..., -7.4851e-01,\n",
      "            7.8589e-02,  8.5038e-01]],\n",
      "\n",
      "         [[-8.9766e-03,  2.6926e-02,  1.5262e-02,  ...,  1.5449e-02,\n",
      "           -3.6062e-02, -7.1460e-02],\n",
      "          [-2.3194e-01,  5.7570e-01, -1.1237e-01,  ..., -8.1351e-01,\n",
      "           -6.6976e-02,  3.2252e-01]],\n",
      "\n",
      "         [[ 1.0204e-02, -4.6890e-02, -2.9096e-02,  ..., -3.9735e-02,\n",
      "           -5.7886e-02,  5.9986e-03],\n",
      "          [ 4.7702e-01, -4.7063e-01, -3.3871e-01,  ...,  2.1069e-01,\n",
      "            8.3466e-02, -5.5454e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6375e-03,  1.4208e-02, -1.0759e-02,  ..., -2.4281e-02,\n",
      "           -1.8885e-03, -4.2574e-02],\n",
      "          [-1.8297e-01, -2.4112e-01,  3.6010e-01,  ...,  5.8115e-01,\n",
      "           -1.0154e-01, -5.8482e-01]],\n",
      "\n",
      "         [[ 1.8735e-02,  3.7347e-02, -1.9538e-02,  ..., -1.3072e-02,\n",
      "           -8.7511e-03, -2.9711e-02],\n",
      "          [-8.5228e-01, -5.2895e-01, -6.2494e-01,  ...,  1.5450e+00,\n",
      "           -6.5782e-01, -7.4831e-02]],\n",
      "\n",
      "         [[-2.3241e-02, -2.2612e-02,  5.0321e-03,  ..., -1.6250e-02,\n",
      "           -9.9282e-03, -2.1594e-03],\n",
      "          [-7.0114e-01,  1.7526e-01, -1.7402e-01,  ..., -1.4011e+00,\n",
      "           -2.8275e-01, -1.2053e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.4696e-01,  6.0328e-02,  1.0008e-01,  ...,  1.6260e-01,\n",
      "           -9.6997e-02, -8.3914e-01],\n",
      "          [ 9.3473e-01, -7.9716e-01,  4.5912e-01,  ...,  8.5994e+00,\n",
      "           -6.2049e+00,  2.7049e+00]],\n",
      "\n",
      "         [[-8.4772e-03, -8.8555e-02, -2.3590e-02,  ...,  1.1478e+00,\n",
      "           -2.2458e+00, -2.2704e+00],\n",
      "          [ 8.9470e-01, -7.1845e-01,  9.0344e-02,  ..., -1.9954e+00,\n",
      "            1.4097e+00,  4.7540e+00]],\n",
      "\n",
      "         [[ 3.1115e-02, -1.7218e-02,  2.6684e-02,  ...,  1.4649e-01,\n",
      "            5.4738e-01,  6.7309e-01],\n",
      "          [ 2.9114e-01, -1.5368e+00,  2.5371e+00,  ...,  8.6953e-01,\n",
      "            8.9969e-03, -1.5600e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3602e-02, -3.7313e-03, -1.4659e-02,  ..., -9.8422e-02,\n",
      "           -4.4682e-01,  2.3950e-01],\n",
      "          [ 1.1708e+00,  2.4860e-02,  7.9032e-01,  ..., -1.9051e+00,\n",
      "            3.4189e+00, -4.3661e+00]],\n",
      "\n",
      "         [[-5.4956e-02, -1.1083e-01, -2.9253e-02,  ...,  6.2771e-02,\n",
      "            2.5498e-01, -1.1519e-02],\n",
      "          [-8.9187e-01, -9.1081e-01, -5.3709e-01,  ..., -1.7786e-01,\n",
      "            1.3409e+00,  1.5125e-01]],\n",
      "\n",
      "         [[ 1.6385e-02,  4.0096e-02, -8.1026e-02,  ..., -1.7525e-01,\n",
      "           -4.0426e-02,  1.0583e-01],\n",
      "          [-1.3164e+00, -2.2112e-01,  3.2179e-01,  ..., -6.6582e-01,\n",
      "            6.1689e+00, -4.6196e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 6.8182e-03,  2.7356e-02, -2.1790e-02,  ...,  1.6913e-02,\n",
      "           -2.8088e-02, -2.5283e-03],\n",
      "          [ 3.3295e-01,  2.0190e-01,  2.6238e-01,  ...,  3.2758e-01,\n",
      "            2.6133e-01, -1.4400e-01]],\n",
      "\n",
      "         [[-1.7816e-01, -3.2563e-01,  4.9874e-02,  ..., -2.5786e-01,\n",
      "           -2.1769e-02,  9.1273e-02],\n",
      "          [ 4.8969e-01,  1.1472e-01, -2.3325e-01,  ..., -6.3723e-01,\n",
      "            1.0473e+00, -7.0047e-01]],\n",
      "\n",
      "         [[ 3.0300e-02, -1.5520e-02,  3.9303e-01,  ..., -1.4554e-02,\n",
      "           -2.9648e-02, -1.4063e-02],\n",
      "          [-5.1066e-02, -3.4841e-01, -2.5457e-02,  ..., -3.8193e-01,\n",
      "            8.2463e-02,  1.3407e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.9096e-03, -2.0108e-02, -2.7415e-02,  ..., -4.9384e-02,\n",
      "           -2.3887e-02, -3.1393e-02],\n",
      "          [-3.0247e-01,  1.2773e+00,  3.3094e-01,  ...,  5.8947e-01,\n",
      "            2.7061e-01, -1.5330e-02]],\n",
      "\n",
      "         [[ 2.2890e-02, -5.6978e-06,  9.3043e-02,  ...,  9.1686e-03,\n",
      "            5.4902e-02,  2.7431e-02],\n",
      "          [ 1.0504e-01,  4.4756e-01,  1.9962e-01,  ..., -9.7831e-01,\n",
      "           -7.1680e-01,  2.8756e-01]],\n",
      "\n",
      "         [[-8.5240e-03,  1.8919e-02, -4.6135e-02,  ..., -2.8233e-04,\n",
      "           -1.1686e-02, -4.7458e-02],\n",
      "          [ 4.6187e-02, -5.6321e-01,  5.0861e-01,  ...,  1.3822e-01,\n",
      "           -9.4153e-02, -3.4030e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.4428e-02,  6.2717e-02, -6.1205e-02,  ...,  6.5594e-02,\n",
      "            5.5240e-02, -9.6419e-02],\n",
      "          [-3.6624e-01, -8.9001e-01, -4.5087e-01,  ..., -3.2815e-01,\n",
      "           -1.7843e-01, -5.4986e-01]],\n",
      "\n",
      "         [[ 3.4703e-02, -3.2534e-02,  1.7036e-03,  ..., -1.0556e-01,\n",
      "            5.6700e-01,  2.5018e-01],\n",
      "          [-5.7426e-02, -3.3931e-01, -1.0338e-02,  ...,  2.0314e+00,\n",
      "            6.1100e-01, -1.7795e+00]],\n",
      "\n",
      "         [[-4.6731e-02, -5.1421e-03, -4.1595e-02,  ...,  2.1222e-01,\n",
      "           -5.6646e-02,  2.6610e-01],\n",
      "          [-7.1356e-02, -4.5361e-01,  5.8922e-01,  ...,  7.1400e+00,\n",
      "            8.9345e-01, -5.3730e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5388e-02, -1.4575e-02,  2.7545e-02,  ...,  1.6023e-01,\n",
      "           -1.9964e-01,  1.4158e-01],\n",
      "          [ 1.5994e+00,  1.1532e+00,  1.8665e+00,  ...,  7.3732e-01,\n",
      "            1.3873e-01, -1.0907e+00]],\n",
      "\n",
      "         [[-4.8309e-03,  2.8271e-01, -3.2916e-03,  ...,  9.1707e-02,\n",
      "            1.9773e-01,  3.9602e-02],\n",
      "          [-1.3726e+00,  6.6538e-02, -3.4821e-01,  ...,  1.3798e-01,\n",
      "           -1.3592e+00,  3.2263e+00]],\n",
      "\n",
      "         [[ 4.0984e-02,  2.7011e-03, -1.6451e-04,  ...,  2.0312e-01,\n",
      "           -1.1621e-01,  1.0360e-01],\n",
      "          [ 2.1593e+00, -4.2072e+00,  1.5861e+00,  ..., -7.3080e-02,\n",
      "           -2.4072e-01, -5.3779e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-6.4428e-02,  3.5687e-02, -2.1046e-03,  ...,  4.2250e-02,\n",
      "            5.4431e-02, -7.8879e-02],\n",
      "          [-3.0762e-01, -1.3985e-01, -4.8119e-02,  ..., -1.5382e-01,\n",
      "            5.3187e-01, -2.6644e-01]],\n",
      "\n",
      "         [[-1.9700e-02, -4.8484e-02,  3.3514e-02,  ...,  6.6231e-03,\n",
      "           -4.4029e-02,  3.5954e-03],\n",
      "          [ 8.6667e-02, -1.8534e-01, -4.0154e-01,  ..., -3.0940e-01,\n",
      "           -3.9454e-01, -3.9856e-01]],\n",
      "\n",
      "         [[ 1.9662e-02,  1.6009e-02, -1.1136e-02,  ...,  2.2426e-02,\n",
      "           -4.6493e-02,  8.4055e-04],\n",
      "          [ 1.1710e-02, -4.6887e-01, -6.9135e-01,  ..., -3.7348e-01,\n",
      "           -4.0064e-01,  6.7406e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.3837e-03, -5.7671e-02,  4.3586e-03,  ..., -6.0560e-02,\n",
      "            4.2681e-02,  2.8576e-01],\n",
      "          [-9.9084e-01,  2.8212e-01, -2.1419e-01,  ..., -2.3755e-01,\n",
      "           -2.7644e-01, -4.5482e-01]],\n",
      "\n",
      "         [[ 1.4662e-01, -6.4527e-02, -1.0372e-02,  ..., -2.8160e-02,\n",
      "           -2.3195e-02, -1.2571e-02],\n",
      "          [-7.9092e-01, -6.6591e-01,  3.7749e-01,  ..., -6.5293e-01,\n",
      "           -1.9721e-01,  6.5413e-01]],\n",
      "\n",
      "         [[ 5.0042e-02,  3.9216e-02,  1.7985e-02,  ...,  9.5383e-03,\n",
      "            1.8801e-02, -4.8135e-02],\n",
      "          [ 5.4319e-02,  1.1363e+00,  2.9854e-01,  ..., -1.1008e-01,\n",
      "           -5.9483e-01, -2.8280e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.1543e-02,  1.4418e-01, -2.0077e-02,  ...,  4.0192e-01,\n",
      "            2.3398e-01,  7.2531e-02],\n",
      "          [ 4.3485e-01, -1.7873e-01, -5.1311e-01,  ..., -9.1109e-01,\n",
      "           -1.4075e+00, -1.3587e+00]],\n",
      "\n",
      "         [[ 1.0457e-01, -8.2028e-02,  5.6426e-02,  ..., -1.1533e-02,\n",
      "           -7.8770e-01,  1.7688e-01],\n",
      "          [ 7.4253e-01, -4.7585e-01,  1.2931e+00,  ...,  1.0803e+00,\n",
      "            2.2327e+00, -3.7756e-01]],\n",
      "\n",
      "         [[-1.7596e-02,  7.7413e-03, -4.5784e-02,  ..., -4.4233e-02,\n",
      "           -1.0818e-01, -3.6405e+00],\n",
      "          [-6.9653e-02,  6.1497e-01, -5.8109e-01,  ...,  5.2200e-01,\n",
      "           -9.8177e-01,  7.0798e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0897e-02, -1.0593e-03, -3.2708e-02,  ..., -1.1109e-02,\n",
      "            9.8196e-02, -4.1893e-01],\n",
      "          [-5.3743e-01, -4.5919e-01,  1.9866e-01,  ..., -6.1939e-01,\n",
      "           -1.7785e-01, -6.4168e+00]],\n",
      "\n",
      "         [[-2.3990e-02,  5.1960e-02,  9.7870e-02,  ..., -5.7983e-02,\n",
      "           -2.0424e-01, -8.6436e-02],\n",
      "          [-1.6256e-01,  4.1263e-01,  7.3683e-02,  ..., -1.9569e+00,\n",
      "           -4.5914e+00, -7.2861e-01]],\n",
      "\n",
      "         [[-3.7772e-02, -5.4814e-03,  8.8471e-02,  ..., -8.6658e-02,\n",
      "            1.9315e-02,  2.3646e-01],\n",
      "          [ 9.1712e-03,  5.7685e-01,  1.8219e-01,  ..., -3.7606e+00,\n",
      "            1.9052e+00,  1.1523e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-2.9771e-02, -3.0148e-02,  6.0639e-02,  ..., -6.3588e-03,\n",
      "           -5.6928e-02, -5.1305e-02],\n",
      "          [ 4.3513e-01, -4.9176e-01, -4.5931e-01,  ..., -3.6536e-01,\n",
      "            4.5780e-01, -3.5848e-01]],\n",
      "\n",
      "         [[ 6.7551e-03,  2.8186e-02, -1.5575e-02,  ..., -7.1470e-03,\n",
      "           -1.9164e-02,  6.2885e-02],\n",
      "          [-3.3106e-01,  1.7938e+00,  9.4642e-02,  ...,  3.3837e-01,\n",
      "            8.5301e-01,  1.9574e-01]],\n",
      "\n",
      "         [[ 1.5036e-02, -1.0191e-02, -2.1293e-02,  ...,  4.5791e-02,\n",
      "            7.1026e-03,  2.2069e-02],\n",
      "          [-6.4547e-02, -6.1387e-01,  8.6606e-02,  ..., -2.1741e-01,\n",
      "           -3.4836e-01, -3.9911e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.5478e-03,  4.0826e-02,  2.8852e-02,  ...,  7.9158e-04,\n",
      "           -6.6088e-02, -1.7784e-02],\n",
      "          [ 4.4400e-01, -3.5788e-01, -4.1872e-01,  ...,  9.8999e-01,\n",
      "           -8.4299e-02,  7.8876e-02]],\n",
      "\n",
      "         [[-6.6095e-04,  1.0722e-02, -6.8633e-03,  ..., -4.5701e-01,\n",
      "            4.9495e-03, -2.2442e-02],\n",
      "          [-2.0063e-01, -5.7115e-01, -1.2169e+00,  ...,  2.2791e+00,\n",
      "            3.5183e-01, -5.8005e-01]],\n",
      "\n",
      "         [[ 1.6312e-02,  3.3529e-02,  1.7517e-02,  ..., -3.3047e-03,\n",
      "           -5.1448e-03,  3.7935e-03],\n",
      "          [ 3.3059e-01,  3.4694e-02, -1.3674e+00,  ...,  8.1314e-01,\n",
      "           -8.0331e-01,  4.5589e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-3.1020e-02,  4.0470e-02,  2.3716e-03,  ..., -8.3137e-01,\n",
      "           -1.2267e+00, -6.7254e-01],\n",
      "          [ 3.0594e-02,  7.9475e-01, -5.1515e-03,  ..., -3.2797e-01,\n",
      "            1.8077e+00, -1.9694e+00]],\n",
      "\n",
      "         [[ 7.4963e-03, -6.0455e-02, -8.6568e-02,  ...,  2.4224e-01,\n",
      "           -5.8448e-02, -1.4898e-01],\n",
      "          [ 2.4471e-01,  5.0797e-02, -5.7775e-01,  ...,  1.6645e+00,\n",
      "           -1.0312e+01, -4.6739e+00]],\n",
      "\n",
      "         [[-7.4445e-03,  3.2921e-02, -2.3530e-02,  ..., -7.2646e-03,\n",
      "            1.6557e-01,  6.2023e-02],\n",
      "          [-4.0989e-02, -4.6313e-01,  1.9761e-01,  ...,  2.3781e+00,\n",
      "           -4.0889e+00, -3.2940e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5687e-01,  1.4893e-02,  4.9250e-02,  ...,  1.0933e-01,\n",
      "           -1.8203e-01, -3.2795e-01],\n",
      "          [ 1.6665e-01,  3.7338e-01,  1.1157e+00,  ..., -1.2241e+00,\n",
      "            2.9547e-02, -1.1550e+00]],\n",
      "\n",
      "         [[-1.2328e-02,  3.2286e-02, -6.3656e-02,  ...,  1.4334e-01,\n",
      "            3.0678e-01, -6.3266e-01],\n",
      "          [ 3.6318e-01,  3.7628e-01,  3.9229e-01,  ..., -2.6422e+00,\n",
      "           -3.2045e+00,  1.7932e+00]],\n",
      "\n",
      "         [[ 2.7699e-02, -5.9582e-02,  1.3748e-04,  ..., -8.2497e-01,\n",
      "            2.8155e-01,  1.7398e+00],\n",
      "          [ 1.7751e-02, -5.4729e-01,  4.7710e-01,  ...,  4.3999e+00,\n",
      "            2.3444e+00,  1.8443e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0138, -0.0331, -0.0352,  ...,  0.0286, -0.0238, -0.0462],\n",
      "          [ 0.7435,  0.6616, -0.4822,  ..., -0.5472, -0.0377,  0.4148]],\n",
      "\n",
      "         [[ 0.0322,  0.1473,  0.0116,  ...,  0.0206, -0.0254,  0.0077],\n",
      "          [-0.2281,  0.2644,  0.9504,  ..., -0.3162, -0.6861, -0.6277]],\n",
      "\n",
      "         [[-0.0952, -0.0032,  0.0873,  ..., -0.0646, -0.0044, -0.0138],\n",
      "          [-0.1782,  0.4692, -0.7005,  ...,  0.1698, -0.1838, -0.0438]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0483,  0.0185,  0.0066,  ...,  0.0418, -0.0234,  0.0028],\n",
      "          [-1.8490,  0.2815,  1.0431,  ...,  0.1769,  0.9086, -0.6404]],\n",
      "\n",
      "         [[ 0.0066,  0.0274,  0.3112,  ...,  0.0271,  0.0229, -0.0675],\n",
      "          [-0.7570, -0.1363, -1.6135,  ...,  0.5165,  1.0947,  0.3839]],\n",
      "\n",
      "         [[ 0.0277,  0.0525, -0.0280,  ...,  0.0699, -0.0427,  0.0961],\n",
      "          [-0.3478,  0.7098,  0.3338,  ...,  0.5723,  0.5121, -0.3605]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.6204e-02, -2.1178e-02,  1.5814e-02,  ...,  8.5867e-02,\n",
      "           -8.9474e-02,  2.1608e-01],\n",
      "          [ 1.9896e+00,  1.0126e+00, -6.4802e-01,  ..., -1.7667e-01,\n",
      "           -5.5300e-01, -1.5126e-01]],\n",
      "\n",
      "         [[ 1.4768e-01,  4.6344e-02, -5.1982e-03,  ..., -1.6492e-01,\n",
      "           -2.1908e-01, -7.2737e-01],\n",
      "          [-3.1403e-02,  1.9861e-01,  1.2207e-01,  ..., -1.9361e+00,\n",
      "            1.0849e+00,  2.5182e+00]],\n",
      "\n",
      "         [[ 3.5666e-02,  1.6266e-02, -7.7979e-02,  ..., -2.6858e-01,\n",
      "           -2.6359e-01,  1.0612e-01],\n",
      "          [ 1.8021e+00, -2.4829e+00, -7.8831e-01,  ..., -5.5051e+00,\n",
      "            3.3432e-01, -5.4476e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.1767e-02, -5.0961e-02,  3.1953e-02,  ...,  2.7458e-01,\n",
      "            3.7028e-01,  4.3507e-02],\n",
      "          [ 1.5242e-01, -2.0724e+00, -1.0608e+00,  ..., -5.9408e-01,\n",
      "           -3.8940e+00, -2.4671e+00]],\n",
      "\n",
      "         [[ 7.3611e-02, -1.5217e-01,  2.5931e-02,  ..., -2.0163e-01,\n",
      "           -1.0444e-01,  1.0020e+00],\n",
      "          [ 6.7425e-02, -5.5373e-01,  3.9016e-01,  ...,  1.9676e-03,\n",
      "           -1.0051e+00, -5.7802e+00]],\n",
      "\n",
      "         [[-2.8117e-03,  2.2521e-02, -6.2477e-02,  ..., -8.1411e-02,\n",
      "            3.7109e-01, -1.2437e-01],\n",
      "          [-7.3558e-01, -4.5053e-01, -3.0111e+00,  ...,  1.8567e-01,\n",
      "           -1.4835e-01, -1.0314e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-5.1882e-02, -1.2847e-02, -2.8018e-03,  ..., -1.5669e-02,\n",
      "            3.3019e-02,  2.8167e-02],\n",
      "          [ 6.7938e-01, -1.3804e+00, -1.0624e+00,  ...,  3.8839e-01,\n",
      "           -2.5900e-01,  1.0986e+00]],\n",
      "\n",
      "         [[ 2.2017e-02,  4.3358e-02,  1.2898e-02,  ..., -1.4048e-02,\n",
      "            6.4917e-02, -4.0277e-02],\n",
      "          [ 7.1163e-01,  5.2090e-01, -6.9362e-02,  ...,  6.5221e-01,\n",
      "            3.0722e-01,  2.5890e-01]],\n",
      "\n",
      "         [[ 2.9758e-02,  1.4308e-02,  4.8436e-02,  ..., -9.7135e-03,\n",
      "           -3.9090e-02, -2.2888e-02],\n",
      "          [ 9.3122e-01,  1.2807e-03,  1.3772e+00,  ...,  1.0193e+00,\n",
      "           -5.0277e-01, -5.3220e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.5540e-02,  1.7509e-02,  1.4758e-01,  ..., -1.9743e-02,\n",
      "            8.3937e-03,  3.8332e-02],\n",
      "          [-1.7745e-02,  4.3756e-01, -2.8456e-01,  ...,  5.7836e-01,\n",
      "            7.5073e-01, -1.8878e-01]],\n",
      "\n",
      "         [[ 6.2948e-04, -3.0884e-03, -4.7241e-02,  ...,  1.9848e-02,\n",
      "            1.8586e-02,  2.1518e-01],\n",
      "          [-1.7183e+00,  6.4053e-02,  8.7801e-01,  ...,  1.1589e-01,\n",
      "           -1.7365e-01, -5.8040e-01]],\n",
      "\n",
      "         [[ 8.4344e-03,  5.5359e-02,  2.4415e-02,  ..., -6.3641e-02,\n",
      "           -3.8261e-02, -6.1251e-03],\n",
      "          [-6.7329e-01,  5.4516e-01,  1.3625e-01,  ...,  4.8941e-01,\n",
      "           -4.7983e-01, -9.5368e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.3981e-02, -5.1033e-02,  5.0766e-02,  ..., -5.0062e-01,\n",
      "           -1.0334e-01,  7.9313e-01],\n",
      "          [-3.0193e-01,  3.3407e-01, -4.7792e-01,  ..., -6.1453e+00,\n",
      "           -4.5750e-03, -2.9954e+00]],\n",
      "\n",
      "         [[ 1.1565e-02,  4.2911e-02, -5.8316e-02,  ...,  5.5326e-01,\n",
      "           -5.4803e-01,  7.1386e-02],\n",
      "          [-3.1996e-01, -2.6313e-01, -2.0382e-01,  ...,  6.8668e+00,\n",
      "            1.6514e+00,  2.9518e+00]],\n",
      "\n",
      "         [[ 1.3365e-01, -3.4013e-02,  1.0878e-01,  ..., -6.1439e-01,\n",
      "            1.6705e-01,  4.9394e-01],\n",
      "          [ 4.2648e-01,  1.3397e-01,  5.9020e-01,  ..., -5.3379e+00,\n",
      "            1.1091e+00,  2.4191e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.3644e-02, -7.4904e-02,  1.2620e-02,  ...,  3.4645e-01,\n",
      "            3.0370e-01,  6.5314e-01],\n",
      "          [-5.0150e-01,  4.4460e-01, -3.1822e-01,  ..., -4.1263e+00,\n",
      "            9.6841e-01, -7.0593e-01]],\n",
      "\n",
      "         [[ 2.8772e-02, -8.3631e-02,  1.0468e-01,  ...,  4.8898e-01,\n",
      "            8.3910e-01, -4.7744e-01],\n",
      "          [-7.5676e-01, -4.8187e-01,  7.8735e-01,  ...,  7.6885e-01,\n",
      "            2.7287e+00,  9.9384e-01]],\n",
      "\n",
      "         [[ 9.7489e-02,  1.0032e-01,  6.1871e-02,  ...,  1.0074e-01,\n",
      "            9.1882e-02, -2.3746e-01],\n",
      "          [ 5.4778e-01, -8.4210e-02,  8.0090e-01,  ..., -1.2793e+00,\n",
      "           -2.2876e+00,  5.4065e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0239,  0.0229,  0.0963,  ..., -0.0104,  0.0146, -0.0165],\n",
      "          [-0.2234,  0.1858,  0.5417,  ..., -0.6943,  0.1901, -0.7825]],\n",
      "\n",
      "         [[ 0.0060,  0.0421, -0.0426,  ...,  0.0147, -0.0221, -0.0168],\n",
      "          [-1.8276,  2.1457,  0.2261,  ...,  0.5341,  1.0236, -0.0327]],\n",
      "\n",
      "         [[ 0.0187,  0.0329, -0.0662,  ...,  0.0200, -0.0500, -0.0052],\n",
      "          [ 0.7041,  0.9266,  1.5628,  ..., -1.3666,  0.5541,  1.0166]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0128, -0.0026,  0.0167,  ..., -0.0168,  0.0168, -0.0584],\n",
      "          [ 1.1742,  0.0642, -0.3099,  ...,  0.0553, -0.2641,  0.1393]],\n",
      "\n",
      "         [[ 0.0459,  0.0567, -0.0135,  ..., -0.0142, -0.0593,  0.0248],\n",
      "          [ 0.2208, -0.3192,  0.1119,  ..., -1.1031, -0.4464,  0.3786]],\n",
      "\n",
      "         [[ 0.0214,  0.1409, -0.0745,  ..., -0.0205,  0.0305,  0.0205],\n",
      "          [-0.2729, -1.2064, -0.3762,  ...,  0.4074,  0.6868,  0.2144]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 6.7302e-02, -1.3629e-01,  5.0385e-02,  ..., -9.1744e-02,\n",
      "            1.9657e-01, -4.2955e-01],\n",
      "          [ 1.0445e+00, -3.6891e+00,  2.3173e+00,  ...,  1.9813e+00,\n",
      "            4.9170e-02, -5.3124e-01]],\n",
      "\n",
      "         [[ 2.5980e-02, -5.1927e-04,  6.3804e-03,  ...,  6.6351e-02,\n",
      "            4.2621e-01, -2.4866e-01],\n",
      "          [-2.0342e-01, -4.2218e-01, -2.0078e-01,  ..., -4.1000e-01,\n",
      "           -8.9916e-02,  3.4105e-01]],\n",
      "\n",
      "         [[ 7.9349e-02,  1.2159e-02,  9.2977e-02,  ...,  1.4971e-01,\n",
      "            2.8673e-02,  1.8554e-01],\n",
      "          [ 9.1499e-01, -1.2090e+00,  1.2207e+00,  ..., -1.4405e+00,\n",
      "            9.8565e-01, -1.2149e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3450e-02,  3.2164e-02,  6.5492e-02,  ..., -1.8413e-01,\n",
      "           -7.4282e-01, -9.0047e-01],\n",
      "          [-1.6353e-01,  1.1146e-01, -2.4478e-01,  ..., -5.1500e-01,\n",
      "            1.3738e-01, -5.9981e+00]],\n",
      "\n",
      "         [[ 3.6138e-02,  2.6437e-02,  9.0338e-02,  ..., -4.9451e-01,\n",
      "            1.6792e-01, -6.6265e-01],\n",
      "          [-8.5011e-01,  3.4502e-02, -3.5911e-01,  ..., -5.4892e+00,\n",
      "           -9.5303e-01,  1.3822e-01]],\n",
      "\n",
      "         [[ 2.4044e+00, -5.8991e-01, -4.0199e-02,  ..., -3.3844e+00,\n",
      "           -2.7069e+00, -6.2224e-01],\n",
      "          [-1.2745e+00, -1.2894e-03,  1.7353e+00,  ..., -3.1872e+00,\n",
      "           -1.5288e+00, -1.5213e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-2.1501e-02, -5.6302e-02, -3.5987e-02,  ..., -7.0282e-02,\n",
      "            9.8479e-02,  5.0567e-02],\n",
      "          [ 6.0235e-01,  8.3084e-01, -2.6666e-01,  ..., -1.0625e+00,\n",
      "           -1.9840e-01, -4.5587e-01]],\n",
      "\n",
      "         [[ 2.0729e-02, -3.2882e-02,  7.6697e-02,  ...,  1.4487e-02,\n",
      "           -6.5960e-02, -1.1711e-01],\n",
      "          [ 9.1505e-02,  6.1734e-01,  1.0133e+00,  ..., -4.7715e-01,\n",
      "           -8.7791e-01,  1.3531e-01]],\n",
      "\n",
      "         [[-8.3269e-02,  3.6837e-02, -1.1577e-03,  ...,  2.1720e-02,\n",
      "           -1.1742e-01, -1.2763e-01],\n",
      "          [ 7.6201e-01,  1.1807e+00, -1.9427e-02,  ...,  1.1808e-01,\n",
      "            4.3355e-01,  5.3450e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1183e-01, -9.7368e-03, -4.2593e-02,  ..., -9.1986e-02,\n",
      "            2.4301e-02, -3.4758e-02],\n",
      "          [-4.8373e-01, -1.5379e-02,  1.2318e+00,  ..., -7.1179e-01,\n",
      "           -7.4459e-01,  1.2289e+00]],\n",
      "\n",
      "         [[ 5.2469e-02, -9.5057e-02, -1.4922e-02,  ..., -1.4945e-02,\n",
      "            5.5055e-02, -5.9208e-02],\n",
      "          [ 7.1178e-01,  1.2866e-01, -4.5461e-01,  ...,  6.2712e-01,\n",
      "            1.2166e+00,  1.2401e-01]],\n",
      "\n",
      "         [[ 3.8412e-01,  2.6937e-01,  3.9394e-01,  ...,  5.9600e-01,\n",
      "           -3.2103e-01, -3.7215e-01],\n",
      "          [-2.3797e-01, -2.1917e-01,  8.9082e-01,  ...,  4.6520e-01,\n",
      "           -8.5068e-01,  3.8035e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 9.4774e-02,  7.8121e-02,  1.0218e-01,  ...,  1.7972e-03,\n",
      "            4.0614e+00,  1.6527e-01],\n",
      "          [ 2.6901e-01,  6.4271e-01, -1.7779e-01,  ..., -3.8674e-01,\n",
      "            1.5650e+00,  5.5930e-01]],\n",
      "\n",
      "         [[-2.5779e-03, -5.5323e-02,  4.7628e-02,  ..., -1.7182e-02,\n",
      "            8.0441e-02, -1.6467e+00],\n",
      "          [-2.8083e+00,  2.7605e-02,  1.6211e+00,  ...,  2.4571e+00,\n",
      "           -9.5529e-01,  2.2362e+00]],\n",
      "\n",
      "         [[-1.4768e-01,  1.6992e-01,  6.8547e-02,  ...,  9.5369e-03,\n",
      "           -4.8737e-01, -5.4544e-01],\n",
      "          [ 1.3024e-01,  4.8087e-01,  5.7523e-03,  ...,  7.1782e-01,\n",
      "            4.4715e+00,  1.5042e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5185e-02,  1.6790e-02, -1.3908e-01,  ...,  6.2092e-02,\n",
      "           -2.4852e-01, -1.1935e+00],\n",
      "          [-2.8822e-01, -7.5007e-01, -7.0703e-01,  ...,  1.3631e-01,\n",
      "           -4.3842e-01,  2.4878e+00]],\n",
      "\n",
      "         [[ 4.6772e-04, -5.3534e-03, -5.0871e-02,  ..., -3.0980e-01,\n",
      "            1.9054e-01,  9.9263e-01],\n",
      "          [ 5.9615e-01, -7.6381e-01,  3.2322e-01,  ...,  1.0430e+00,\n",
      "            1.6601e-01, -2.9888e+00]],\n",
      "\n",
      "         [[-1.1110e-01, -1.0173e-01,  2.0782e-01,  ...,  4.5352e-01,\n",
      "           -2.3605e-01,  8.0786e-02],\n",
      "          [ 3.4052e-01, -2.5276e-01,  1.9501e-01,  ...,  4.8229e+00,\n",
      "            2.5878e+00, -2.4809e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 8.8309e-02,  6.8389e-02, -1.3175e-01,  ...,  3.0843e-02,\n",
      "            5.7244e-02,  1.4149e-02],\n",
      "          [ 4.8943e-01, -5.5852e-01,  2.2643e-01,  ..., -7.4936e-01,\n",
      "            1.3597e+00, -5.6667e-01]],\n",
      "\n",
      "         [[ 1.2954e-02,  6.9540e-02,  4.5960e-02,  ...,  5.0308e-02,\n",
      "            7.2022e-02, -4.1003e-02],\n",
      "          [-6.4006e-01, -6.8432e-01, -1.2704e+00,  ...,  1.1377e+00,\n",
      "            1.2328e+00,  2.5358e-01]],\n",
      "\n",
      "         [[ 2.6753e-02,  9.9847e-02,  1.1587e-02,  ...,  3.1947e-02,\n",
      "            2.3067e-02,  3.8148e-04],\n",
      "          [ 7.5214e-01,  3.1664e+00,  2.1909e+00,  ...,  1.7452e+00,\n",
      "            1.0168e+00, -1.4202e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.5586e-02, -1.2875e-02,  1.6793e-02,  ...,  2.3447e-03,\n",
      "            4.0639e-02,  8.2289e-02],\n",
      "          [-1.7721e-01,  3.9944e-01,  2.5186e-01,  ...,  2.0159e-01,\n",
      "           -1.0855e+00,  2.9934e-01]],\n",
      "\n",
      "         [[-9.1333e-02,  6.6431e-02, -2.1712e-02,  ...,  2.2908e-02,\n",
      "            2.1859e-04, -2.6711e-02],\n",
      "          [ 1.5613e+00, -3.3071e-01, -7.1202e-01,  ...,  5.7536e-02,\n",
      "           -6.8519e-01,  3.1957e-01]],\n",
      "\n",
      "         [[ 5.4510e-02, -1.1333e-01,  3.8895e-02,  ..., -3.1386e-02,\n",
      "           -1.1100e-01, -1.6671e-02],\n",
      "          [ 4.5195e-01,  7.2924e-01,  9.7647e-01,  ..., -2.5615e-01,\n",
      "            1.6585e+00,  2.9175e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-5.8685e-03, -1.4420e-01, -1.2216e-01,  ..., -6.2341e-02,\n",
      "            2.6507e-02, -7.3803e-01],\n",
      "          [ 4.0759e-01,  7.1958e-01, -1.4525e+00,  ..., -3.5240e-01,\n",
      "           -1.2981e-01,  1.8858e+00]],\n",
      "\n",
      "         [[-7.7595e-03, -2.5711e-02,  7.5179e-04,  ..., -5.2141e-01,\n",
      "            6.8812e-01, -7.9960e-01],\n",
      "          [ 2.2781e-01, -4.9728e-01, -6.3194e-01,  ..., -6.2643e-01,\n",
      "           -3.0391e+00,  6.8031e-01]],\n",
      "\n",
      "         [[ 4.1518e-02,  1.0930e-01,  1.7378e-01,  ..., -2.0971e-01,\n",
      "            2.1385e-01,  3.0522e-01],\n",
      "          [ 1.6203e+00, -7.8768e-01,  1.6605e+00,  ...,  1.0584e+00,\n",
      "           -1.3450e+00,  7.1019e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4174e-02,  8.5096e-02,  5.0144e-03,  ...,  4.3793e-01,\n",
      "           -4.4822e-01,  8.7011e-02],\n",
      "          [ 8.2225e-02,  4.2790e-02, -4.9961e-02,  ..., -4.0793e+00,\n",
      "           -4.3174e+00, -2.8809e-01]],\n",
      "\n",
      "         [[ 2.6047e-02,  2.2280e-02, -1.5481e-01,  ..., -2.6478e-01,\n",
      "           -1.4109e-01,  5.2302e-01],\n",
      "          [ 3.5084e-01,  3.3457e-01,  5.9025e-01,  ...,  2.7767e+00,\n",
      "           -6.9344e+00, -8.1541e-01]],\n",
      "\n",
      "         [[ 2.3714e-01,  9.5794e-03, -8.2547e-02,  ..., -8.2370e-02,\n",
      "            1.3196e-02,  3.7194e-01],\n",
      "          [ 1.9183e+00, -1.0403e+00, -1.4105e-02,  ...,  3.3592e-01,\n",
      "            1.8738e-01,  8.6685e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 6.2228e-03, -5.3289e-02,  7.0610e-02,  ..., -1.1292e-02,\n",
      "           -4.6136e-02,  1.7962e-01],\n",
      "          [-7.3503e-01, -1.2776e+00,  1.8474e-01,  ..., -1.0276e+00,\n",
      "            9.8285e-01,  3.0630e-01]],\n",
      "\n",
      "         [[-4.9292e-02,  9.4149e-02, -1.3472e-01,  ...,  8.6767e-02,\n",
      "           -2.4149e-02,  8.5394e-02],\n",
      "          [-8.5437e-01, -2.4944e-01,  5.8805e-01,  ...,  1.4545e+00,\n",
      "            2.9201e-02, -6.1744e-01]],\n",
      "\n",
      "         [[ 5.8058e-02, -3.4792e-02, -2.6434e-02,  ..., -1.2326e-01,\n",
      "            9.4212e-03, -5.3683e-02],\n",
      "          [ 2.8530e-02,  8.5335e-01, -2.0517e-01,  ...,  3.2897e-01,\n",
      "            6.4199e-01, -3.2670e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.7441e-02,  1.1978e-01, -1.0631e-02,  ...,  8.5700e-02,\n",
      "            5.7076e-01,  1.4336e-01],\n",
      "          [ 1.0543e+00, -2.4359e-01,  3.7871e-01,  ..., -1.5477e+00,\n",
      "           -8.2385e-01,  1.6532e-01]],\n",
      "\n",
      "         [[ 1.2424e-02, -1.8853e-02,  1.5109e-02,  ..., -3.5732e-02,\n",
      "            4.2184e-02,  6.0508e-04],\n",
      "          [-1.3326e-01, -1.4143e+00, -1.3379e-01,  ...,  3.0377e-01,\n",
      "            1.1823e+00, -6.1828e-01]],\n",
      "\n",
      "         [[ 2.4419e-01, -1.0912e-01,  1.9546e-02,  ...,  1.5495e-01,\n",
      "           -7.0538e-02, -1.1443e-01],\n",
      "          [ 1.0064e+00,  6.2659e-01,  3.1264e-02,  ...,  7.7063e-01,\n",
      "            1.0175e+00, -7.1043e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 6.2397e-02, -1.8932e-01, -1.0471e-01,  ..., -3.9743e-01,\n",
      "           -4.6013e-01, -1.8186e-01],\n",
      "          [ 6.3104e-02, -1.3305e-01,  2.5889e-02,  ...,  1.3180e-03,\n",
      "            1.6916e+00,  7.0048e+00]],\n",
      "\n",
      "         [[-6.0302e-02,  6.6039e-02, -2.9719e-02,  ..., -1.0726e+00,\n",
      "            5.3956e-01,  2.1189e-01],\n",
      "          [-2.8626e-01,  2.5804e-02, -1.1536e+00,  ..., -5.6526e+00,\n",
      "            6.9549e-01, -1.8140e-01]],\n",
      "\n",
      "         [[ 3.5332e-02, -1.1530e-01, -1.4402e-01,  ..., -1.6112e-01,\n",
      "            3.7257e-02,  4.4384e-02],\n",
      "          [ 2.2448e-01, -1.4250e+00, -2.3763e+00,  ...,  1.2723e+00,\n",
      "           -1.4827e+00, -1.3951e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3871e-01,  1.1833e-01, -1.5723e-01,  ...,  6.2954e-01,\n",
      "           -6.2329e-01,  1.2023e-01],\n",
      "          [-1.1738e+00,  1.1706e+00, -5.3189e-01,  ..., -1.4653e+00,\n",
      "           -2.0969e+00,  2.2819e+00]],\n",
      "\n",
      "         [[ 1.1438e-01,  7.4377e-02, -1.0821e-02,  ...,  4.3515e-01,\n",
      "           -3.5927e-01, -1.5075e-01],\n",
      "          [ 1.6845e+00,  1.5047e+00,  6.1408e-01,  ...,  1.2233e+00,\n",
      "            7.2743e-01,  3.8849e-01]],\n",
      "\n",
      "         [[-1.6897e-02, -8.9466e-02,  1.9934e-02,  ...,  4.3671e-01,\n",
      "           -1.0010e+00,  1.7164e-01],\n",
      "          [ 1.0794e-01,  6.9099e-02,  2.5148e-01,  ..., -2.6921e+00,\n",
      "           -6.0855e+00, -7.8068e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0860,  0.0231,  0.0763,  ...,  0.0199, -0.0165, -0.0547],\n",
      "          [ 1.0737,  0.0227,  0.6919,  ..., -0.5879, -1.0900,  0.4663]],\n",
      "\n",
      "         [[ 0.0598,  0.1830,  0.0271,  ..., -0.0200, -0.0768, -0.0722],\n",
      "          [ 0.4824,  0.2657, -1.1995,  ...,  1.0960, -0.0414,  0.2829]],\n",
      "\n",
      "         [[-0.5807,  0.0995, -0.0152,  ..., -0.1114, -0.0062, -0.0399],\n",
      "          [-1.4747,  0.0895,  0.0567,  ...,  0.2233,  0.0407,  0.0313]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0506, -0.0210,  0.0480,  ..., -0.0520, -0.0392, -0.0215],\n",
      "          [-0.6479, -0.6768, -0.8169,  ..., -1.5954,  2.1942, -1.0069]],\n",
      "\n",
      "         [[-0.0174,  0.0117, -0.0394,  ...,  0.0057, -0.1239, -0.0121],\n",
      "          [-0.5528,  1.9728, -0.3986,  ...,  0.6576, -1.3567, -0.2887]],\n",
      "\n",
      "         [[-0.1529,  0.0625, -0.0512,  ...,  0.0369,  0.0976,  0.0956],\n",
      "          [ 1.3794, -1.3308, -2.0352,  ..., -0.0871,  1.3208,  2.7046]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.3796e-01,  2.0522e-01,  2.2101e-01,  ...,  3.2821e+00,\n",
      "           -5.5904e+00,  6.5015e+00],\n",
      "          [-3.2146e+00,  7.1657e-01,  1.8182e-01,  ...,  5.4310e+00,\n",
      "           -1.8628e+00,  3.1901e+00]],\n",
      "\n",
      "         [[ 1.6762e-01, -2.8385e-01,  1.8011e-01,  ...,  5.5672e+00,\n",
      "            1.0379e+00,  1.7009e+00],\n",
      "          [-1.0682e-01,  4.0145e-01, -8.3205e-01,  ...,  5.1541e+00,\n",
      "            6.2055e-01,  1.1282e+00]],\n",
      "\n",
      "         [[ 6.9141e-06, -5.5571e-02, -2.0147e-02,  ..., -6.6001e+00,\n",
      "            1.0624e+00,  1.8985e+00],\n",
      "          [-4.1954e-01,  1.0194e+00,  1.1901e+00,  ..., -1.8375e+00,\n",
      "            2.9291e+00,  1.2704e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.0833e-03,  2.1628e-01,  7.5842e-02,  ..., -4.9411e-01,\n",
      "            9.7286e-02,  6.9797e-02],\n",
      "          [-4.7745e-01,  2.5050e-01,  1.7100e+00,  ..., -1.4342e+00,\n",
      "           -7.8668e-01,  2.2978e+00]],\n",
      "\n",
      "         [[ 1.4298e-01,  8.9395e-02,  1.9518e-01,  ..., -2.0411e+00,\n",
      "           -3.7647e-01,  4.7512e-01],\n",
      "          [-2.6847e-01,  3.2373e-01,  5.4343e-01,  ...,  1.3671e+00,\n",
      "           -3.1576e+00,  1.2113e+00]],\n",
      "\n",
      "         [[-6.8152e-02,  4.9298e-02, -1.1419e-01,  ..., -3.3315e-01,\n",
      "           -6.0620e+00, -4.2905e+00],\n",
      "          [ 3.0645e-01,  3.5790e-01, -2.6007e-01,  ...,  1.6905e+00,\n",
      "           -9.6833e+00, -5.4472e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-5.0286e-01,  2.3395e-02, -2.3102e-01,  ..., -4.3135e-01,\n",
      "            6.4099e-01, -3.2919e-01],\n",
      "          [ 6.7984e-03, -2.2702e-01, -1.1274e+00,  ..., -4.4305e+00,\n",
      "           -3.2357e-01, -2.9353e-01]],\n",
      "\n",
      "         [[-8.2955e+00,  1.1344e+01,  8.2338e+00,  ..., -1.5762e+00,\n",
      "            8.3296e+00,  8.1610e+00],\n",
      "          [ 4.3884e-01, -2.2983e+00, -6.5135e-01,  ..., -1.1310e+00,\n",
      "            4.9986e-02, -1.2896e+00]],\n",
      "\n",
      "         [[ 2.5667e-02,  4.3192e-02, -9.3927e-02,  ..., -3.2233e-01,\n",
      "           -5.0078e-01, -1.3012e-01],\n",
      "          [ 8.4500e-01, -1.9615e-01, -1.5573e+00,  ..., -4.3190e-01,\n",
      "            7.0828e-02, -1.3204e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2344e-02,  3.6831e-02,  1.6248e-04,  ...,  3.5233e-02,\n",
      "           -2.9590e-02, -1.8202e-01],\n",
      "          [-2.8315e-01, -6.6290e-01,  4.0266e-01,  ...,  4.6045e-01,\n",
      "            1.8908e+00, -6.1417e-01]],\n",
      "\n",
      "         [[ 3.6203e-02,  3.4657e-02, -2.1393e-01,  ..., -2.1338e-01,\n",
      "            2.2547e-01,  2.0264e-02],\n",
      "          [-4.1854e-01,  8.1202e-01,  3.7153e-01,  ..., -8.1224e-01,\n",
      "           -3.3161e-02, -2.1392e+00]],\n",
      "\n",
      "         [[-7.5084e+00, -2.9793e+00, -2.8263e+00,  ..., -9.5946e+00,\n",
      "           -1.0108e+01, -1.6785e+00],\n",
      "          [ 5.9964e-01, -9.6979e-01,  3.4527e-01,  ..., -1.2308e+00,\n",
      "            1.1691e+00, -9.9012e-01]]]], grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 使用本地路径加载模型和分词器\n",
    "model_path = \"./model/qwen0.5b\"\n",
    "\n",
    "# 使用AutoModel来加载Qwen模型\n",
    "# AutoModel 是用于加载 Transformer 模型的通用接口，这里加载的是 Qwen-0.5B 模型\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# 使用AutoTokenizer来加载分词器\n",
    "# AutoTokenizer 是用于处理文本输入的工具，它负责将文本转换为模型可以处理的张量格式\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 测试分词器\n",
    "text = \"我喜欢小狗\"  # 输入的示例文本\n",
    "\n",
    "# 将文本转换为模型输入格式（张量），包括词汇的编码和注意力掩码\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 使用模型进行推理，生成输出\n",
    "# 将张量输入到模型，得到输出，包括 last_hidden_state 和 past_key_values\n",
    "outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "# 输出模型的返回结果，包括隐藏状态和缓存的上下文信息\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36b08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2Model(\n",
      "      (embed_tokens): Embedding(151936, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Qwen2DecoderLayer(\n",
      "          (self_attn): Qwen2SdpaAttention(\n",
      "            (q_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (rotary_emb): Qwen2RotaryEmbedding()\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "            (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "            (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "      (rotary_emb): Qwen2RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 定义LoRA的微调配置\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA秩参数\n",
    "    lora_alpha=32,  # LoRA的alpha值，控制学习率\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 需要进行LoRA调节的层\n",
    "    lora_dropout=0.05,  # Dropout比率\n",
    "    bias=\"none\",  # 不使用偏置项\n",
    "    task_type=\"CAUSAL_LM\"  # 任务类型是因果语言模型\n",
    ")\n",
    "\n",
    "# 将LoRA应用于模型\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 打印微调后的模型结构\n",
    "print(lora_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7392bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'answer': 'Saint Bernadette Soubirous', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'context': 'It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.', 'p_phrase': ['Lourdes', 'France', '1858', 'the Virgin Mary', 'appeared'], 'n_phrase': ['Saint Bernadette Soubirous'], 'full answer': 'The Virgin Mary allegedly appeared in 1858 in Lourdes France to Saint Bernadette Soubirous.'}, {'id': 1, 'answer': 'a copper statue of Christ', 'question': 'What is in front of the Notre Dame Main Building?', 'context': 'Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".', 'p_phrase': ['the Main Building', 'Christ', 'front'], 'n_phrase': ['a copper statue', 'facing'], 'full answer': 'A copper statue of Christ is in front of the Notre Dame Main Building.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 读取自定义的JSON格式问答数据集\n",
    "with open('./data/SQuAD1.1/train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 查看数据集前两个样本结构\n",
    "print(data[:2])\n",
    "\n",
    "# 定义自定义数据集类，用于Pytorch中的数据加载\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        inputs = self.tokenizer(sample[\"context\"], sample[\"question\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": self.tokenizer(sample[\"answer\"], return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# 加载数据集\n",
    "dataset = QADataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303e65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 自定义数据集类，将数据转换为PyTorch张量\n",
    "class SQuADDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            item[\"question\"], \n",
    "            item[\"context\"], \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(item[\"answer\"], dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# 实例化数据集\n",
    "dataset = SQuADDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28be74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embed_tokens\n",
      "layers\n",
      "layers.0\n",
      "layers.0.self_attn\n",
      "layers.0.self_attn.q_proj\n",
      "layers.0.self_attn.q_proj.base_layer\n",
      "layers.0.self_attn.q_proj.lora_dropout\n",
      "layers.0.self_attn.q_proj.lora_dropout.default\n",
      "layers.0.self_attn.q_proj.lora_A\n",
      "layers.0.self_attn.q_proj.lora_A.default\n",
      "layers.0.self_attn.q_proj.lora_B\n",
      "layers.0.self_attn.q_proj.lora_B.default\n",
      "layers.0.self_attn.q_proj.lora_embedding_A\n",
      "layers.0.self_attn.q_proj.lora_embedding_B\n",
      "layers.0.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.0.self_attn.k_proj\n",
      "layers.0.self_attn.v_proj\n",
      "layers.0.self_attn.v_proj.base_layer\n",
      "layers.0.self_attn.v_proj.lora_dropout\n",
      "layers.0.self_attn.v_proj.lora_dropout.default\n",
      "layers.0.self_attn.v_proj.lora_A\n",
      "layers.0.self_attn.v_proj.lora_A.default\n",
      "layers.0.self_attn.v_proj.lora_B\n",
      "layers.0.self_attn.v_proj.lora_B.default\n",
      "layers.0.self_attn.v_proj.lora_embedding_A\n",
      "layers.0.self_attn.v_proj.lora_embedding_B\n",
      "layers.0.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.0.self_attn.o_proj\n",
      "layers.0.self_attn.rotary_emb\n",
      "layers.0.mlp\n",
      "layers.0.mlp.gate_proj\n",
      "layers.0.mlp.up_proj\n",
      "layers.0.mlp.down_proj\n",
      "layers.0.mlp.act_fn\n",
      "layers.0.input_layernorm\n",
      "layers.0.post_attention_layernorm\n",
      "layers.1\n",
      "layers.1.self_attn\n",
      "layers.1.self_attn.q_proj\n",
      "layers.1.self_attn.q_proj.base_layer\n",
      "layers.1.self_attn.q_proj.lora_dropout\n",
      "layers.1.self_attn.q_proj.lora_dropout.default\n",
      "layers.1.self_attn.q_proj.lora_A\n",
      "layers.1.self_attn.q_proj.lora_A.default\n",
      "layers.1.self_attn.q_proj.lora_B\n",
      "layers.1.self_attn.q_proj.lora_B.default\n",
      "layers.1.self_attn.q_proj.lora_embedding_A\n",
      "layers.1.self_attn.q_proj.lora_embedding_B\n",
      "layers.1.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.1.self_attn.k_proj\n",
      "layers.1.self_attn.v_proj\n",
      "layers.1.self_attn.v_proj.base_layer\n",
      "layers.1.self_attn.v_proj.lora_dropout\n",
      "layers.1.self_attn.v_proj.lora_dropout.default\n",
      "layers.1.self_attn.v_proj.lora_A\n",
      "layers.1.self_attn.v_proj.lora_A.default\n",
      "layers.1.self_attn.v_proj.lora_B\n",
      "layers.1.self_attn.v_proj.lora_B.default\n",
      "layers.1.self_attn.v_proj.lora_embedding_A\n",
      "layers.1.self_attn.v_proj.lora_embedding_B\n",
      "layers.1.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.1.self_attn.o_proj\n",
      "layers.1.self_attn.rotary_emb\n",
      "layers.1.mlp\n",
      "layers.1.mlp.gate_proj\n",
      "layers.1.mlp.up_proj\n",
      "layers.1.mlp.down_proj\n",
      "layers.1.mlp.act_fn\n",
      "layers.1.input_layernorm\n",
      "layers.1.post_attention_layernorm\n",
      "layers.2\n",
      "layers.2.self_attn\n",
      "layers.2.self_attn.q_proj\n",
      "layers.2.self_attn.q_proj.base_layer\n",
      "layers.2.self_attn.q_proj.lora_dropout\n",
      "layers.2.self_attn.q_proj.lora_dropout.default\n",
      "layers.2.self_attn.q_proj.lora_A\n",
      "layers.2.self_attn.q_proj.lora_A.default\n",
      "layers.2.self_attn.q_proj.lora_B\n",
      "layers.2.self_attn.q_proj.lora_B.default\n",
      "layers.2.self_attn.q_proj.lora_embedding_A\n",
      "layers.2.self_attn.q_proj.lora_embedding_B\n",
      "layers.2.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.2.self_attn.k_proj\n",
      "layers.2.self_attn.v_proj\n",
      "layers.2.self_attn.v_proj.base_layer\n",
      "layers.2.self_attn.v_proj.lora_dropout\n",
      "layers.2.self_attn.v_proj.lora_dropout.default\n",
      "layers.2.self_attn.v_proj.lora_A\n",
      "layers.2.self_attn.v_proj.lora_A.default\n",
      "layers.2.self_attn.v_proj.lora_B\n",
      "layers.2.self_attn.v_proj.lora_B.default\n",
      "layers.2.self_attn.v_proj.lora_embedding_A\n",
      "layers.2.self_attn.v_proj.lora_embedding_B\n",
      "layers.2.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.2.self_attn.o_proj\n",
      "layers.2.self_attn.rotary_emb\n",
      "layers.2.mlp\n",
      "layers.2.mlp.gate_proj\n",
      "layers.2.mlp.up_proj\n",
      "layers.2.mlp.down_proj\n",
      "layers.2.mlp.act_fn\n",
      "layers.2.input_layernorm\n",
      "layers.2.post_attention_layernorm\n",
      "layers.3\n",
      "layers.3.self_attn\n",
      "layers.3.self_attn.q_proj\n",
      "layers.3.self_attn.q_proj.base_layer\n",
      "layers.3.self_attn.q_proj.lora_dropout\n",
      "layers.3.self_attn.q_proj.lora_dropout.default\n",
      "layers.3.self_attn.q_proj.lora_A\n",
      "layers.3.self_attn.q_proj.lora_A.default\n",
      "layers.3.self_attn.q_proj.lora_B\n",
      "layers.3.self_attn.q_proj.lora_B.default\n",
      "layers.3.self_attn.q_proj.lora_embedding_A\n",
      "layers.3.self_attn.q_proj.lora_embedding_B\n",
      "layers.3.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.3.self_attn.k_proj\n",
      "layers.3.self_attn.v_proj\n",
      "layers.3.self_attn.v_proj.base_layer\n",
      "layers.3.self_attn.v_proj.lora_dropout\n",
      "layers.3.self_attn.v_proj.lora_dropout.default\n",
      "layers.3.self_attn.v_proj.lora_A\n",
      "layers.3.self_attn.v_proj.lora_A.default\n",
      "layers.3.self_attn.v_proj.lora_B\n",
      "layers.3.self_attn.v_proj.lora_B.default\n",
      "layers.3.self_attn.v_proj.lora_embedding_A\n",
      "layers.3.self_attn.v_proj.lora_embedding_B\n",
      "layers.3.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.3.self_attn.o_proj\n",
      "layers.3.self_attn.rotary_emb\n",
      "layers.3.mlp\n",
      "layers.3.mlp.gate_proj\n",
      "layers.3.mlp.up_proj\n",
      "layers.3.mlp.down_proj\n",
      "layers.3.mlp.act_fn\n",
      "layers.3.input_layernorm\n",
      "layers.3.post_attention_layernorm\n",
      "layers.4\n",
      "layers.4.self_attn\n",
      "layers.4.self_attn.q_proj\n",
      "layers.4.self_attn.q_proj.base_layer\n",
      "layers.4.self_attn.q_proj.lora_dropout\n",
      "layers.4.self_attn.q_proj.lora_dropout.default\n",
      "layers.4.self_attn.q_proj.lora_A\n",
      "layers.4.self_attn.q_proj.lora_A.default\n",
      "layers.4.self_attn.q_proj.lora_B\n",
      "layers.4.self_attn.q_proj.lora_B.default\n",
      "layers.4.self_attn.q_proj.lora_embedding_A\n",
      "layers.4.self_attn.q_proj.lora_embedding_B\n",
      "layers.4.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.4.self_attn.k_proj\n",
      "layers.4.self_attn.v_proj\n",
      "layers.4.self_attn.v_proj.base_layer\n",
      "layers.4.self_attn.v_proj.lora_dropout\n",
      "layers.4.self_attn.v_proj.lora_dropout.default\n",
      "layers.4.self_attn.v_proj.lora_A\n",
      "layers.4.self_attn.v_proj.lora_A.default\n",
      "layers.4.self_attn.v_proj.lora_B\n",
      "layers.4.self_attn.v_proj.lora_B.default\n",
      "layers.4.self_attn.v_proj.lora_embedding_A\n",
      "layers.4.self_attn.v_proj.lora_embedding_B\n",
      "layers.4.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.4.self_attn.o_proj\n",
      "layers.4.self_attn.rotary_emb\n",
      "layers.4.mlp\n",
      "layers.4.mlp.gate_proj\n",
      "layers.4.mlp.up_proj\n",
      "layers.4.mlp.down_proj\n",
      "layers.4.mlp.act_fn\n",
      "layers.4.input_layernorm\n",
      "layers.4.post_attention_layernorm\n",
      "layers.5\n",
      "layers.5.self_attn\n",
      "layers.5.self_attn.q_proj\n",
      "layers.5.self_attn.q_proj.base_layer\n",
      "layers.5.self_attn.q_proj.lora_dropout\n",
      "layers.5.self_attn.q_proj.lora_dropout.default\n",
      "layers.5.self_attn.q_proj.lora_A\n",
      "layers.5.self_attn.q_proj.lora_A.default\n",
      "layers.5.self_attn.q_proj.lora_B\n",
      "layers.5.self_attn.q_proj.lora_B.default\n",
      "layers.5.self_attn.q_proj.lora_embedding_A\n",
      "layers.5.self_attn.q_proj.lora_embedding_B\n",
      "layers.5.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.5.self_attn.k_proj\n",
      "layers.5.self_attn.v_proj\n",
      "layers.5.self_attn.v_proj.base_layer\n",
      "layers.5.self_attn.v_proj.lora_dropout\n",
      "layers.5.self_attn.v_proj.lora_dropout.default\n",
      "layers.5.self_attn.v_proj.lora_A\n",
      "layers.5.self_attn.v_proj.lora_A.default\n",
      "layers.5.self_attn.v_proj.lora_B\n",
      "layers.5.self_attn.v_proj.lora_B.default\n",
      "layers.5.self_attn.v_proj.lora_embedding_A\n",
      "layers.5.self_attn.v_proj.lora_embedding_B\n",
      "layers.5.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.5.self_attn.o_proj\n",
      "layers.5.self_attn.rotary_emb\n",
      "layers.5.mlp\n",
      "layers.5.mlp.gate_proj\n",
      "layers.5.mlp.up_proj\n",
      "layers.5.mlp.down_proj\n",
      "layers.5.mlp.act_fn\n",
      "layers.5.input_layernorm\n",
      "layers.5.post_attention_layernorm\n",
      "layers.6\n",
      "layers.6.self_attn\n",
      "layers.6.self_attn.q_proj\n",
      "layers.6.self_attn.q_proj.base_layer\n",
      "layers.6.self_attn.q_proj.lora_dropout\n",
      "layers.6.self_attn.q_proj.lora_dropout.default\n",
      "layers.6.self_attn.q_proj.lora_A\n",
      "layers.6.self_attn.q_proj.lora_A.default\n",
      "layers.6.self_attn.q_proj.lora_B\n",
      "layers.6.self_attn.q_proj.lora_B.default\n",
      "layers.6.self_attn.q_proj.lora_embedding_A\n",
      "layers.6.self_attn.q_proj.lora_embedding_B\n",
      "layers.6.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.6.self_attn.k_proj\n",
      "layers.6.self_attn.v_proj\n",
      "layers.6.self_attn.v_proj.base_layer\n",
      "layers.6.self_attn.v_proj.lora_dropout\n",
      "layers.6.self_attn.v_proj.lora_dropout.default\n",
      "layers.6.self_attn.v_proj.lora_A\n",
      "layers.6.self_attn.v_proj.lora_A.default\n",
      "layers.6.self_attn.v_proj.lora_B\n",
      "layers.6.self_attn.v_proj.lora_B.default\n",
      "layers.6.self_attn.v_proj.lora_embedding_A\n",
      "layers.6.self_attn.v_proj.lora_embedding_B\n",
      "layers.6.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.6.self_attn.o_proj\n",
      "layers.6.self_attn.rotary_emb\n",
      "layers.6.mlp\n",
      "layers.6.mlp.gate_proj\n",
      "layers.6.mlp.up_proj\n",
      "layers.6.mlp.down_proj\n",
      "layers.6.mlp.act_fn\n",
      "layers.6.input_layernorm\n",
      "layers.6.post_attention_layernorm\n",
      "layers.7\n",
      "layers.7.self_attn\n",
      "layers.7.self_attn.q_proj\n",
      "layers.7.self_attn.q_proj.base_layer\n",
      "layers.7.self_attn.q_proj.lora_dropout\n",
      "layers.7.self_attn.q_proj.lora_dropout.default\n",
      "layers.7.self_attn.q_proj.lora_A\n",
      "layers.7.self_attn.q_proj.lora_A.default\n",
      "layers.7.self_attn.q_proj.lora_B\n",
      "layers.7.self_attn.q_proj.lora_B.default\n",
      "layers.7.self_attn.q_proj.lora_embedding_A\n",
      "layers.7.self_attn.q_proj.lora_embedding_B\n",
      "layers.7.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.7.self_attn.k_proj\n",
      "layers.7.self_attn.v_proj\n",
      "layers.7.self_attn.v_proj.base_layer\n",
      "layers.7.self_attn.v_proj.lora_dropout\n",
      "layers.7.self_attn.v_proj.lora_dropout.default\n",
      "layers.7.self_attn.v_proj.lora_A\n",
      "layers.7.self_attn.v_proj.lora_A.default\n",
      "layers.7.self_attn.v_proj.lora_B\n",
      "layers.7.self_attn.v_proj.lora_B.default\n",
      "layers.7.self_attn.v_proj.lora_embedding_A\n",
      "layers.7.self_attn.v_proj.lora_embedding_B\n",
      "layers.7.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.7.self_attn.o_proj\n",
      "layers.7.self_attn.rotary_emb\n",
      "layers.7.mlp\n",
      "layers.7.mlp.gate_proj\n",
      "layers.7.mlp.up_proj\n",
      "layers.7.mlp.down_proj\n",
      "layers.7.mlp.act_fn\n",
      "layers.7.input_layernorm\n",
      "layers.7.post_attention_layernorm\n",
      "layers.8\n",
      "layers.8.self_attn\n",
      "layers.8.self_attn.q_proj\n",
      "layers.8.self_attn.q_proj.base_layer\n",
      "layers.8.self_attn.q_proj.lora_dropout\n",
      "layers.8.self_attn.q_proj.lora_dropout.default\n",
      "layers.8.self_attn.q_proj.lora_A\n",
      "layers.8.self_attn.q_proj.lora_A.default\n",
      "layers.8.self_attn.q_proj.lora_B\n",
      "layers.8.self_attn.q_proj.lora_B.default\n",
      "layers.8.self_attn.q_proj.lora_embedding_A\n",
      "layers.8.self_attn.q_proj.lora_embedding_B\n",
      "layers.8.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.8.self_attn.k_proj\n",
      "layers.8.self_attn.v_proj\n",
      "layers.8.self_attn.v_proj.base_layer\n",
      "layers.8.self_attn.v_proj.lora_dropout\n",
      "layers.8.self_attn.v_proj.lora_dropout.default\n",
      "layers.8.self_attn.v_proj.lora_A\n",
      "layers.8.self_attn.v_proj.lora_A.default\n",
      "layers.8.self_attn.v_proj.lora_B\n",
      "layers.8.self_attn.v_proj.lora_B.default\n",
      "layers.8.self_attn.v_proj.lora_embedding_A\n",
      "layers.8.self_attn.v_proj.lora_embedding_B\n",
      "layers.8.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.8.self_attn.o_proj\n",
      "layers.8.self_attn.rotary_emb\n",
      "layers.8.mlp\n",
      "layers.8.mlp.gate_proj\n",
      "layers.8.mlp.up_proj\n",
      "layers.8.mlp.down_proj\n",
      "layers.8.mlp.act_fn\n",
      "layers.8.input_layernorm\n",
      "layers.8.post_attention_layernorm\n",
      "layers.9\n",
      "layers.9.self_attn\n",
      "layers.9.self_attn.q_proj\n",
      "layers.9.self_attn.q_proj.base_layer\n",
      "layers.9.self_attn.q_proj.lora_dropout\n",
      "layers.9.self_attn.q_proj.lora_dropout.default\n",
      "layers.9.self_attn.q_proj.lora_A\n",
      "layers.9.self_attn.q_proj.lora_A.default\n",
      "layers.9.self_attn.q_proj.lora_B\n",
      "layers.9.self_attn.q_proj.lora_B.default\n",
      "layers.9.self_attn.q_proj.lora_embedding_A\n",
      "layers.9.self_attn.q_proj.lora_embedding_B\n",
      "layers.9.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.9.self_attn.k_proj\n",
      "layers.9.self_attn.v_proj\n",
      "layers.9.self_attn.v_proj.base_layer\n",
      "layers.9.self_attn.v_proj.lora_dropout\n",
      "layers.9.self_attn.v_proj.lora_dropout.default\n",
      "layers.9.self_attn.v_proj.lora_A\n",
      "layers.9.self_attn.v_proj.lora_A.default\n",
      "layers.9.self_attn.v_proj.lora_B\n",
      "layers.9.self_attn.v_proj.lora_B.default\n",
      "layers.9.self_attn.v_proj.lora_embedding_A\n",
      "layers.9.self_attn.v_proj.lora_embedding_B\n",
      "layers.9.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.9.self_attn.o_proj\n",
      "layers.9.self_attn.rotary_emb\n",
      "layers.9.mlp\n",
      "layers.9.mlp.gate_proj\n",
      "layers.9.mlp.up_proj\n",
      "layers.9.mlp.down_proj\n",
      "layers.9.mlp.act_fn\n",
      "layers.9.input_layernorm\n",
      "layers.9.post_attention_layernorm\n",
      "layers.10\n",
      "layers.10.self_attn\n",
      "layers.10.self_attn.q_proj\n",
      "layers.10.self_attn.q_proj.base_layer\n",
      "layers.10.self_attn.q_proj.lora_dropout\n",
      "layers.10.self_attn.q_proj.lora_dropout.default\n",
      "layers.10.self_attn.q_proj.lora_A\n",
      "layers.10.self_attn.q_proj.lora_A.default\n",
      "layers.10.self_attn.q_proj.lora_B\n",
      "layers.10.self_attn.q_proj.lora_B.default\n",
      "layers.10.self_attn.q_proj.lora_embedding_A\n",
      "layers.10.self_attn.q_proj.lora_embedding_B\n",
      "layers.10.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.10.self_attn.k_proj\n",
      "layers.10.self_attn.v_proj\n",
      "layers.10.self_attn.v_proj.base_layer\n",
      "layers.10.self_attn.v_proj.lora_dropout\n",
      "layers.10.self_attn.v_proj.lora_dropout.default\n",
      "layers.10.self_attn.v_proj.lora_A\n",
      "layers.10.self_attn.v_proj.lora_A.default\n",
      "layers.10.self_attn.v_proj.lora_B\n",
      "layers.10.self_attn.v_proj.lora_B.default\n",
      "layers.10.self_attn.v_proj.lora_embedding_A\n",
      "layers.10.self_attn.v_proj.lora_embedding_B\n",
      "layers.10.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.10.self_attn.o_proj\n",
      "layers.10.self_attn.rotary_emb\n",
      "layers.10.mlp\n",
      "layers.10.mlp.gate_proj\n",
      "layers.10.mlp.up_proj\n",
      "layers.10.mlp.down_proj\n",
      "layers.10.mlp.act_fn\n",
      "layers.10.input_layernorm\n",
      "layers.10.post_attention_layernorm\n",
      "layers.11\n",
      "layers.11.self_attn\n",
      "layers.11.self_attn.q_proj\n",
      "layers.11.self_attn.q_proj.base_layer\n",
      "layers.11.self_attn.q_proj.lora_dropout\n",
      "layers.11.self_attn.q_proj.lora_dropout.default\n",
      "layers.11.self_attn.q_proj.lora_A\n",
      "layers.11.self_attn.q_proj.lora_A.default\n",
      "layers.11.self_attn.q_proj.lora_B\n",
      "layers.11.self_attn.q_proj.lora_B.default\n",
      "layers.11.self_attn.q_proj.lora_embedding_A\n",
      "layers.11.self_attn.q_proj.lora_embedding_B\n",
      "layers.11.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.11.self_attn.k_proj\n",
      "layers.11.self_attn.v_proj\n",
      "layers.11.self_attn.v_proj.base_layer\n",
      "layers.11.self_attn.v_proj.lora_dropout\n",
      "layers.11.self_attn.v_proj.lora_dropout.default\n",
      "layers.11.self_attn.v_proj.lora_A\n",
      "layers.11.self_attn.v_proj.lora_A.default\n",
      "layers.11.self_attn.v_proj.lora_B\n",
      "layers.11.self_attn.v_proj.lora_B.default\n",
      "layers.11.self_attn.v_proj.lora_embedding_A\n",
      "layers.11.self_attn.v_proj.lora_embedding_B\n",
      "layers.11.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.11.self_attn.o_proj\n",
      "layers.11.self_attn.rotary_emb\n",
      "layers.11.mlp\n",
      "layers.11.mlp.gate_proj\n",
      "layers.11.mlp.up_proj\n",
      "layers.11.mlp.down_proj\n",
      "layers.11.mlp.act_fn\n",
      "layers.11.input_layernorm\n",
      "layers.11.post_attention_layernorm\n",
      "layers.12\n",
      "layers.12.self_attn\n",
      "layers.12.self_attn.q_proj\n",
      "layers.12.self_attn.q_proj.base_layer\n",
      "layers.12.self_attn.q_proj.lora_dropout\n",
      "layers.12.self_attn.q_proj.lora_dropout.default\n",
      "layers.12.self_attn.q_proj.lora_A\n",
      "layers.12.self_attn.q_proj.lora_A.default\n",
      "layers.12.self_attn.q_proj.lora_B\n",
      "layers.12.self_attn.q_proj.lora_B.default\n",
      "layers.12.self_attn.q_proj.lora_embedding_A\n",
      "layers.12.self_attn.q_proj.lora_embedding_B\n",
      "layers.12.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.12.self_attn.k_proj\n",
      "layers.12.self_attn.v_proj\n",
      "layers.12.self_attn.v_proj.base_layer\n",
      "layers.12.self_attn.v_proj.lora_dropout\n",
      "layers.12.self_attn.v_proj.lora_dropout.default\n",
      "layers.12.self_attn.v_proj.lora_A\n",
      "layers.12.self_attn.v_proj.lora_A.default\n",
      "layers.12.self_attn.v_proj.lora_B\n",
      "layers.12.self_attn.v_proj.lora_B.default\n",
      "layers.12.self_attn.v_proj.lora_embedding_A\n",
      "layers.12.self_attn.v_proj.lora_embedding_B\n",
      "layers.12.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.12.self_attn.o_proj\n",
      "layers.12.self_attn.rotary_emb\n",
      "layers.12.mlp\n",
      "layers.12.mlp.gate_proj\n",
      "layers.12.mlp.up_proj\n",
      "layers.12.mlp.down_proj\n",
      "layers.12.mlp.act_fn\n",
      "layers.12.input_layernorm\n",
      "layers.12.post_attention_layernorm\n",
      "layers.13\n",
      "layers.13.self_attn\n",
      "layers.13.self_attn.q_proj\n",
      "layers.13.self_attn.q_proj.base_layer\n",
      "layers.13.self_attn.q_proj.lora_dropout\n",
      "layers.13.self_attn.q_proj.lora_dropout.default\n",
      "layers.13.self_attn.q_proj.lora_A\n",
      "layers.13.self_attn.q_proj.lora_A.default\n",
      "layers.13.self_attn.q_proj.lora_B\n",
      "layers.13.self_attn.q_proj.lora_B.default\n",
      "layers.13.self_attn.q_proj.lora_embedding_A\n",
      "layers.13.self_attn.q_proj.lora_embedding_B\n",
      "layers.13.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.13.self_attn.k_proj\n",
      "layers.13.self_attn.v_proj\n",
      "layers.13.self_attn.v_proj.base_layer\n",
      "layers.13.self_attn.v_proj.lora_dropout\n",
      "layers.13.self_attn.v_proj.lora_dropout.default\n",
      "layers.13.self_attn.v_proj.lora_A\n",
      "layers.13.self_attn.v_proj.lora_A.default\n",
      "layers.13.self_attn.v_proj.lora_B\n",
      "layers.13.self_attn.v_proj.lora_B.default\n",
      "layers.13.self_attn.v_proj.lora_embedding_A\n",
      "layers.13.self_attn.v_proj.lora_embedding_B\n",
      "layers.13.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.13.self_attn.o_proj\n",
      "layers.13.self_attn.rotary_emb\n",
      "layers.13.mlp\n",
      "layers.13.mlp.gate_proj\n",
      "layers.13.mlp.up_proj\n",
      "layers.13.mlp.down_proj\n",
      "layers.13.mlp.act_fn\n",
      "layers.13.input_layernorm\n",
      "layers.13.post_attention_layernorm\n",
      "layers.14\n",
      "layers.14.self_attn\n",
      "layers.14.self_attn.q_proj\n",
      "layers.14.self_attn.q_proj.base_layer\n",
      "layers.14.self_attn.q_proj.lora_dropout\n",
      "layers.14.self_attn.q_proj.lora_dropout.default\n",
      "layers.14.self_attn.q_proj.lora_A\n",
      "layers.14.self_attn.q_proj.lora_A.default\n",
      "layers.14.self_attn.q_proj.lora_B\n",
      "layers.14.self_attn.q_proj.lora_B.default\n",
      "layers.14.self_attn.q_proj.lora_embedding_A\n",
      "layers.14.self_attn.q_proj.lora_embedding_B\n",
      "layers.14.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.14.self_attn.k_proj\n",
      "layers.14.self_attn.v_proj\n",
      "layers.14.self_attn.v_proj.base_layer\n",
      "layers.14.self_attn.v_proj.lora_dropout\n",
      "layers.14.self_attn.v_proj.lora_dropout.default\n",
      "layers.14.self_attn.v_proj.lora_A\n",
      "layers.14.self_attn.v_proj.lora_A.default\n",
      "layers.14.self_attn.v_proj.lora_B\n",
      "layers.14.self_attn.v_proj.lora_B.default\n",
      "layers.14.self_attn.v_proj.lora_embedding_A\n",
      "layers.14.self_attn.v_proj.lora_embedding_B\n",
      "layers.14.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.14.self_attn.o_proj\n",
      "layers.14.self_attn.rotary_emb\n",
      "layers.14.mlp\n",
      "layers.14.mlp.gate_proj\n",
      "layers.14.mlp.up_proj\n",
      "layers.14.mlp.down_proj\n",
      "layers.14.mlp.act_fn\n",
      "layers.14.input_layernorm\n",
      "layers.14.post_attention_layernorm\n",
      "layers.15\n",
      "layers.15.self_attn\n",
      "layers.15.self_attn.q_proj\n",
      "layers.15.self_attn.q_proj.base_layer\n",
      "layers.15.self_attn.q_proj.lora_dropout\n",
      "layers.15.self_attn.q_proj.lora_dropout.default\n",
      "layers.15.self_attn.q_proj.lora_A\n",
      "layers.15.self_attn.q_proj.lora_A.default\n",
      "layers.15.self_attn.q_proj.lora_B\n",
      "layers.15.self_attn.q_proj.lora_B.default\n",
      "layers.15.self_attn.q_proj.lora_embedding_A\n",
      "layers.15.self_attn.q_proj.lora_embedding_B\n",
      "layers.15.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.15.self_attn.k_proj\n",
      "layers.15.self_attn.v_proj\n",
      "layers.15.self_attn.v_proj.base_layer\n",
      "layers.15.self_attn.v_proj.lora_dropout\n",
      "layers.15.self_attn.v_proj.lora_dropout.default\n",
      "layers.15.self_attn.v_proj.lora_A\n",
      "layers.15.self_attn.v_proj.lora_A.default\n",
      "layers.15.self_attn.v_proj.lora_B\n",
      "layers.15.self_attn.v_proj.lora_B.default\n",
      "layers.15.self_attn.v_proj.lora_embedding_A\n",
      "layers.15.self_attn.v_proj.lora_embedding_B\n",
      "layers.15.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.15.self_attn.o_proj\n",
      "layers.15.self_attn.rotary_emb\n",
      "layers.15.mlp\n",
      "layers.15.mlp.gate_proj\n",
      "layers.15.mlp.up_proj\n",
      "layers.15.mlp.down_proj\n",
      "layers.15.mlp.act_fn\n",
      "layers.15.input_layernorm\n",
      "layers.15.post_attention_layernorm\n",
      "layers.16\n",
      "layers.16.self_attn\n",
      "layers.16.self_attn.q_proj\n",
      "layers.16.self_attn.q_proj.base_layer\n",
      "layers.16.self_attn.q_proj.lora_dropout\n",
      "layers.16.self_attn.q_proj.lora_dropout.default\n",
      "layers.16.self_attn.q_proj.lora_A\n",
      "layers.16.self_attn.q_proj.lora_A.default\n",
      "layers.16.self_attn.q_proj.lora_B\n",
      "layers.16.self_attn.q_proj.lora_B.default\n",
      "layers.16.self_attn.q_proj.lora_embedding_A\n",
      "layers.16.self_attn.q_proj.lora_embedding_B\n",
      "layers.16.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.16.self_attn.k_proj\n",
      "layers.16.self_attn.v_proj\n",
      "layers.16.self_attn.v_proj.base_layer\n",
      "layers.16.self_attn.v_proj.lora_dropout\n",
      "layers.16.self_attn.v_proj.lora_dropout.default\n",
      "layers.16.self_attn.v_proj.lora_A\n",
      "layers.16.self_attn.v_proj.lora_A.default\n",
      "layers.16.self_attn.v_proj.lora_B\n",
      "layers.16.self_attn.v_proj.lora_B.default\n",
      "layers.16.self_attn.v_proj.lora_embedding_A\n",
      "layers.16.self_attn.v_proj.lora_embedding_B\n",
      "layers.16.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.16.self_attn.o_proj\n",
      "layers.16.self_attn.rotary_emb\n",
      "layers.16.mlp\n",
      "layers.16.mlp.gate_proj\n",
      "layers.16.mlp.up_proj\n",
      "layers.16.mlp.down_proj\n",
      "layers.16.mlp.act_fn\n",
      "layers.16.input_layernorm\n",
      "layers.16.post_attention_layernorm\n",
      "layers.17\n",
      "layers.17.self_attn\n",
      "layers.17.self_attn.q_proj\n",
      "layers.17.self_attn.q_proj.base_layer\n",
      "layers.17.self_attn.q_proj.lora_dropout\n",
      "layers.17.self_attn.q_proj.lora_dropout.default\n",
      "layers.17.self_attn.q_proj.lora_A\n",
      "layers.17.self_attn.q_proj.lora_A.default\n",
      "layers.17.self_attn.q_proj.lora_B\n",
      "layers.17.self_attn.q_proj.lora_B.default\n",
      "layers.17.self_attn.q_proj.lora_embedding_A\n",
      "layers.17.self_attn.q_proj.lora_embedding_B\n",
      "layers.17.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.17.self_attn.k_proj\n",
      "layers.17.self_attn.v_proj\n",
      "layers.17.self_attn.v_proj.base_layer\n",
      "layers.17.self_attn.v_proj.lora_dropout\n",
      "layers.17.self_attn.v_proj.lora_dropout.default\n",
      "layers.17.self_attn.v_proj.lora_A\n",
      "layers.17.self_attn.v_proj.lora_A.default\n",
      "layers.17.self_attn.v_proj.lora_B\n",
      "layers.17.self_attn.v_proj.lora_B.default\n",
      "layers.17.self_attn.v_proj.lora_embedding_A\n",
      "layers.17.self_attn.v_proj.lora_embedding_B\n",
      "layers.17.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.17.self_attn.o_proj\n",
      "layers.17.self_attn.rotary_emb\n",
      "layers.17.mlp\n",
      "layers.17.mlp.gate_proj\n",
      "layers.17.mlp.up_proj\n",
      "layers.17.mlp.down_proj\n",
      "layers.17.mlp.act_fn\n",
      "layers.17.input_layernorm\n",
      "layers.17.post_attention_layernorm\n",
      "layers.18\n",
      "layers.18.self_attn\n",
      "layers.18.self_attn.q_proj\n",
      "layers.18.self_attn.q_proj.base_layer\n",
      "layers.18.self_attn.q_proj.lora_dropout\n",
      "layers.18.self_attn.q_proj.lora_dropout.default\n",
      "layers.18.self_attn.q_proj.lora_A\n",
      "layers.18.self_attn.q_proj.lora_A.default\n",
      "layers.18.self_attn.q_proj.lora_B\n",
      "layers.18.self_attn.q_proj.lora_B.default\n",
      "layers.18.self_attn.q_proj.lora_embedding_A\n",
      "layers.18.self_attn.q_proj.lora_embedding_B\n",
      "layers.18.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.18.self_attn.k_proj\n",
      "layers.18.self_attn.v_proj\n",
      "layers.18.self_attn.v_proj.base_layer\n",
      "layers.18.self_attn.v_proj.lora_dropout\n",
      "layers.18.self_attn.v_proj.lora_dropout.default\n",
      "layers.18.self_attn.v_proj.lora_A\n",
      "layers.18.self_attn.v_proj.lora_A.default\n",
      "layers.18.self_attn.v_proj.lora_B\n",
      "layers.18.self_attn.v_proj.lora_B.default\n",
      "layers.18.self_attn.v_proj.lora_embedding_A\n",
      "layers.18.self_attn.v_proj.lora_embedding_B\n",
      "layers.18.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.18.self_attn.o_proj\n",
      "layers.18.self_attn.rotary_emb\n",
      "layers.18.mlp\n",
      "layers.18.mlp.gate_proj\n",
      "layers.18.mlp.up_proj\n",
      "layers.18.mlp.down_proj\n",
      "layers.18.mlp.act_fn\n",
      "layers.18.input_layernorm\n",
      "layers.18.post_attention_layernorm\n",
      "layers.19\n",
      "layers.19.self_attn\n",
      "layers.19.self_attn.q_proj\n",
      "layers.19.self_attn.q_proj.base_layer\n",
      "layers.19.self_attn.q_proj.lora_dropout\n",
      "layers.19.self_attn.q_proj.lora_dropout.default\n",
      "layers.19.self_attn.q_proj.lora_A\n",
      "layers.19.self_attn.q_proj.lora_A.default\n",
      "layers.19.self_attn.q_proj.lora_B\n",
      "layers.19.self_attn.q_proj.lora_B.default\n",
      "layers.19.self_attn.q_proj.lora_embedding_A\n",
      "layers.19.self_attn.q_proj.lora_embedding_B\n",
      "layers.19.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.19.self_attn.k_proj\n",
      "layers.19.self_attn.v_proj\n",
      "layers.19.self_attn.v_proj.base_layer\n",
      "layers.19.self_attn.v_proj.lora_dropout\n",
      "layers.19.self_attn.v_proj.lora_dropout.default\n",
      "layers.19.self_attn.v_proj.lora_A\n",
      "layers.19.self_attn.v_proj.lora_A.default\n",
      "layers.19.self_attn.v_proj.lora_B\n",
      "layers.19.self_attn.v_proj.lora_B.default\n",
      "layers.19.self_attn.v_proj.lora_embedding_A\n",
      "layers.19.self_attn.v_proj.lora_embedding_B\n",
      "layers.19.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.19.self_attn.o_proj\n",
      "layers.19.self_attn.rotary_emb\n",
      "layers.19.mlp\n",
      "layers.19.mlp.gate_proj\n",
      "layers.19.mlp.up_proj\n",
      "layers.19.mlp.down_proj\n",
      "layers.19.mlp.act_fn\n",
      "layers.19.input_layernorm\n",
      "layers.19.post_attention_layernorm\n",
      "layers.20\n",
      "layers.20.self_attn\n",
      "layers.20.self_attn.q_proj\n",
      "layers.20.self_attn.q_proj.base_layer\n",
      "layers.20.self_attn.q_proj.lora_dropout\n",
      "layers.20.self_attn.q_proj.lora_dropout.default\n",
      "layers.20.self_attn.q_proj.lora_A\n",
      "layers.20.self_attn.q_proj.lora_A.default\n",
      "layers.20.self_attn.q_proj.lora_B\n",
      "layers.20.self_attn.q_proj.lora_B.default\n",
      "layers.20.self_attn.q_proj.lora_embedding_A\n",
      "layers.20.self_attn.q_proj.lora_embedding_B\n",
      "layers.20.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.20.self_attn.k_proj\n",
      "layers.20.self_attn.v_proj\n",
      "layers.20.self_attn.v_proj.base_layer\n",
      "layers.20.self_attn.v_proj.lora_dropout\n",
      "layers.20.self_attn.v_proj.lora_dropout.default\n",
      "layers.20.self_attn.v_proj.lora_A\n",
      "layers.20.self_attn.v_proj.lora_A.default\n",
      "layers.20.self_attn.v_proj.lora_B\n",
      "layers.20.self_attn.v_proj.lora_B.default\n",
      "layers.20.self_attn.v_proj.lora_embedding_A\n",
      "layers.20.self_attn.v_proj.lora_embedding_B\n",
      "layers.20.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.20.self_attn.o_proj\n",
      "layers.20.self_attn.rotary_emb\n",
      "layers.20.mlp\n",
      "layers.20.mlp.gate_proj\n",
      "layers.20.mlp.up_proj\n",
      "layers.20.mlp.down_proj\n",
      "layers.20.mlp.act_fn\n",
      "layers.20.input_layernorm\n",
      "layers.20.post_attention_layernorm\n",
      "layers.21\n",
      "layers.21.self_attn\n",
      "layers.21.self_attn.q_proj\n",
      "layers.21.self_attn.q_proj.base_layer\n",
      "layers.21.self_attn.q_proj.lora_dropout\n",
      "layers.21.self_attn.q_proj.lora_dropout.default\n",
      "layers.21.self_attn.q_proj.lora_A\n",
      "layers.21.self_attn.q_proj.lora_A.default\n",
      "layers.21.self_attn.q_proj.lora_B\n",
      "layers.21.self_attn.q_proj.lora_B.default\n",
      "layers.21.self_attn.q_proj.lora_embedding_A\n",
      "layers.21.self_attn.q_proj.lora_embedding_B\n",
      "layers.21.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.21.self_attn.k_proj\n",
      "layers.21.self_attn.v_proj\n",
      "layers.21.self_attn.v_proj.base_layer\n",
      "layers.21.self_attn.v_proj.lora_dropout\n",
      "layers.21.self_attn.v_proj.lora_dropout.default\n",
      "layers.21.self_attn.v_proj.lora_A\n",
      "layers.21.self_attn.v_proj.lora_A.default\n",
      "layers.21.self_attn.v_proj.lora_B\n",
      "layers.21.self_attn.v_proj.lora_B.default\n",
      "layers.21.self_attn.v_proj.lora_embedding_A\n",
      "layers.21.self_attn.v_proj.lora_embedding_B\n",
      "layers.21.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.21.self_attn.o_proj\n",
      "layers.21.self_attn.rotary_emb\n",
      "layers.21.mlp\n",
      "layers.21.mlp.gate_proj\n",
      "layers.21.mlp.up_proj\n",
      "layers.21.mlp.down_proj\n",
      "layers.21.mlp.act_fn\n",
      "layers.21.input_layernorm\n",
      "layers.21.post_attention_layernorm\n",
      "layers.22\n",
      "layers.22.self_attn\n",
      "layers.22.self_attn.q_proj\n",
      "layers.22.self_attn.q_proj.base_layer\n",
      "layers.22.self_attn.q_proj.lora_dropout\n",
      "layers.22.self_attn.q_proj.lora_dropout.default\n",
      "layers.22.self_attn.q_proj.lora_A\n",
      "layers.22.self_attn.q_proj.lora_A.default\n",
      "layers.22.self_attn.q_proj.lora_B\n",
      "layers.22.self_attn.q_proj.lora_B.default\n",
      "layers.22.self_attn.q_proj.lora_embedding_A\n",
      "layers.22.self_attn.q_proj.lora_embedding_B\n",
      "layers.22.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.22.self_attn.k_proj\n",
      "layers.22.self_attn.v_proj\n",
      "layers.22.self_attn.v_proj.base_layer\n",
      "layers.22.self_attn.v_proj.lora_dropout\n",
      "layers.22.self_attn.v_proj.lora_dropout.default\n",
      "layers.22.self_attn.v_proj.lora_A\n",
      "layers.22.self_attn.v_proj.lora_A.default\n",
      "layers.22.self_attn.v_proj.lora_B\n",
      "layers.22.self_attn.v_proj.lora_B.default\n",
      "layers.22.self_attn.v_proj.lora_embedding_A\n",
      "layers.22.self_attn.v_proj.lora_embedding_B\n",
      "layers.22.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.22.self_attn.o_proj\n",
      "layers.22.self_attn.rotary_emb\n",
      "layers.22.mlp\n",
      "layers.22.mlp.gate_proj\n",
      "layers.22.mlp.up_proj\n",
      "layers.22.mlp.down_proj\n",
      "layers.22.mlp.act_fn\n",
      "layers.22.input_layernorm\n",
      "layers.22.post_attention_layernorm\n",
      "layers.23\n",
      "layers.23.self_attn\n",
      "layers.23.self_attn.q_proj\n",
      "layers.23.self_attn.q_proj.base_layer\n",
      "layers.23.self_attn.q_proj.lora_dropout\n",
      "layers.23.self_attn.q_proj.lora_dropout.default\n",
      "layers.23.self_attn.q_proj.lora_A\n",
      "layers.23.self_attn.q_proj.lora_A.default\n",
      "layers.23.self_attn.q_proj.lora_B\n",
      "layers.23.self_attn.q_proj.lora_B.default\n",
      "layers.23.self_attn.q_proj.lora_embedding_A\n",
      "layers.23.self_attn.q_proj.lora_embedding_B\n",
      "layers.23.self_attn.q_proj.lora_magnitude_vector\n",
      "layers.23.self_attn.k_proj\n",
      "layers.23.self_attn.v_proj\n",
      "layers.23.self_attn.v_proj.base_layer\n",
      "layers.23.self_attn.v_proj.lora_dropout\n",
      "layers.23.self_attn.v_proj.lora_dropout.default\n",
      "layers.23.self_attn.v_proj.lora_A\n",
      "layers.23.self_attn.v_proj.lora_A.default\n",
      "layers.23.self_attn.v_proj.lora_B\n",
      "layers.23.self_attn.v_proj.lora_B.default\n",
      "layers.23.self_attn.v_proj.lora_embedding_A\n",
      "layers.23.self_attn.v_proj.lora_embedding_B\n",
      "layers.23.self_attn.v_proj.lora_magnitude_vector\n",
      "layers.23.self_attn.o_proj\n",
      "layers.23.self_attn.rotary_emb\n",
      "layers.23.mlp\n",
      "layers.23.mlp.gate_proj\n",
      "layers.23.mlp.up_proj\n",
      "layers.23.mlp.down_proj\n",
      "layers.23.mlp.act_fn\n",
      "layers.23.input_layernorm\n",
      "layers.23.post_attention_layernorm\n",
      "norm\n",
      "rotary_emb\n"
     ]
    }
   ],
   "source": [
    "# 打印模型的所有模块\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b9b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_path = \"./model/qwen0.5b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d531b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
